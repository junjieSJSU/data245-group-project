{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn xgboost lightgbm psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj3KQ9NZ1e7m",
        "outputId": "28421331-42ac-4822-a93a-c59a6d6550cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dont change the code. Add code to show memwory usage\n",
        "\n",
        "# Working\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split # Still useful for potential validation split later, but not for initial train/test load\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier # Added SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier # Added BaggingClassifier (as an example of bagging)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB # Added Gaussian Naive Bayes\n",
        "from sklearn.neural_network import MLPClassifier # Added MLP Classifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import time\n",
        "import io # Keep io import in case needed for other things\n",
        "import pickle # Import the pickle library\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "# --- Memory Usage Monitoring Helper ---\n",
        "process = psutil.Process(os.getpid())\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Returns current memory usage of the process in MB.\"\"\"\n",
        "    mem_info = process.memory_info()\n",
        "    return mem_info.rss / (1024 * 1024) # Convert bytes to MB\n",
        "\n",
        "print(f\"Initial memory usage: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "# Import Gradient Boosting Libraries (you might need to install these: pip install xgboost lightgbm)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBClassifier = xgb.XGBClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: XGBoost not installed. Skipping XGBoost model.\")\n",
        "    XGBClassifier = None # Set to None if not installed\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGBMClassifier = lgb.LGBMClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: LightGBM not installed. Skipping LightGBM model.\")\n",
        "    LGBMClassifier = None # Set to None if not installed\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace with the actual paths to your dataset files\n",
        "TRAIN_DATASET_PATH = './train_data.csv' # <--- Path to your training CSV file\n",
        "TEST_DATASET_PATH = './test_data.csv'   # <--- Path to your testing CSV file\n",
        "TARGET_COLUMN = 'Label' # The name of the target column\n",
        "\n",
        "# --- 1. Load Data from Separate CSV Files ---\n",
        "print(f\"\\nMemory usage before loading data: {get_memory_usage():.2f} MB\")\n",
        "try:\n",
        "    df_train = pd.read_csv(TRAIN_DATASET_PATH)\n",
        "    print(f\"Training dataset loaded successfully from {TRAIN_DATASET_PATH}\")\n",
        "    print(f\"Training dataset shape: {df_train.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the training dataset:\")\n",
        "    print(df_train.head())\n",
        "    print(\"\\nTraining Dataset Info:\")\n",
        "    df_train.info()\n",
        "    print(f\"Memory usage after loading training data: {get_memory_usage():.2f} MB\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Training dataset not found at {TRAIN_DATASET_PATH}\")\n",
        "    print(\"Please update TRAIN_DATASET_PATH with the correct path to your training CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "try:\n",
        "    df_test = pd.read_csv(TEST_DATASET_PATH)\n",
        "    print(f\"\\nTesting dataset loaded successfully from {TEST_DATASET_PATH}\")\n",
        "    print(f\"Testing dataset shape: {df_test.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the testing dataset:\")\n",
        "    print(df_test.head())\n",
        "    print(\"\\nTesting Dataset Info:\")\n",
        "    df_test.info()\n",
        "    print(f\"Memory usage after loading testing data: {get_memory_usage():.2f} MB\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Testing dataset not found at {TEST_DATASET_PATH}\")\n",
        "    print(\"Please update TEST_DATASET_PATH with the correct path to your testing CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "\n",
        "# --- 2. Preprocessing ---\n",
        "print(f\"\\nMemory usage before preprocessing: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "# Define columns to drop - apply to both train and test sets\n",
        "columns_to_drop = ['FlowID', 'SourceIP', 'DestinationIP', 'Timestamp']\n",
        "\n",
        "# Apply dropping columns and handling infinite/NaN values to both dataframes\n",
        "def preprocess_dataframe(df, columns_to_drop, target_column):\n",
        "    df_processed = df.drop(columns=columns_to_drop, errors='ignore').copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    # Handle potential infinite values\n",
        "    df_processed.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
        "    # Fill NaN values. Using 0 here, but consider other strategies.\n",
        "    df_processed.fillna(0, inplace=True)\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    X = df_processed.drop(columns=[target_column])\n",
        "    y = df_processed[target_column]\n",
        "\n",
        "    # --- Added: Ensure target column is treated as string for LabelEncoder ---\n",
        "    y = y.astype(str)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_dataframe(df_train, columns_to_drop, TARGET_COLUMN)\n",
        "X_test, y_test = preprocess_dataframe(df_test, columns_to_drop, TARGET_COLUMN)\n",
        "\n",
        "print(\"\\nPreprocessing applied to both training and testing datasets.\")\n",
        "print(f\"Memory usage after initial preprocessing: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "# --- NEW: Convert multi-class target into binary labels: 'BENIGN' vs 'ATTACK' ---\n",
        "def simplify_labels(y):\n",
        "    return y.apply(lambda x: 'BENIGN' if x.upper() == 'BENIGN' else 'ATTACK')\n",
        "\n",
        "y_train = simplify_labels(y_train)\n",
        "y_test = simplify_labels(y_test)\n",
        "\n",
        "print(\"\\nLabels simplified to 'BENIGN' vs 'ATTACK'.\")\n",
        "print(f\"Memory usage after simplifying labels: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "# Encode the target variable if it's categorical (e.g., 'BENIGN', 'ATTACK_TYPE')\n",
        "# Fit on training labels, transform both training and testing labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test) # Use transform, not fit_transform on test set\n",
        "\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"Encoded value {i} -> Class label '{class_name}'\")\n",
        "\n",
        "print(f\"\\nOriginal Training Labels: {y_train.unique()}\")\n",
        "print(f\"Encoded Training Labels Classes: {label_encoder.classes_}\")\n",
        "print(f\"Encoded Training Labels: {label_encoder}\")\n",
        "print(f\"\\nOriginal Testing Labels: {y_test.unique()}\")\n",
        "print(f\"Encoded Testing Labels (based on training labels): {label_encoder.classes_}\")\n",
        "\n",
        "# --- Save the LabelEncoder ---\n",
        "try:\n",
        "    encoder_filename = 'label_encoder.pkl'\n",
        "    with open(encoder_filename, 'wb') as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "    print(f\"\\nLabel encoder saved to {encoder_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving label encoder: {e}\")\n",
        "print(f\"Memory usage after encoding labels and saving encoder: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "# Scale numerical features\n",
        "# Fit scaler ONLY on training data, then transform both train and test data\n",
        "scaler = StandardScaler()\n",
        "# Check if training data is not empty before scaling\n",
        "if X_train.shape[0] > 0:\n",
        "    print(f\"\\nMemory usage before scaling features: {get_memory_usage():.2f} MB\")\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    print(f\"Features scaled using StandardScaler (fitted on training data).\")\n",
        "    print(f\"Memory usage after scaling features: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "    # --- Save the StandardScaler ---\n",
        "    try:\n",
        "        scaler_filename = 'scaler.pkl'\n",
        "        with open(scaler_filename, 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        print(f\"Standard scaler saved to {scaler_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving standard scaler: {e}\")\n",
        "    print(f\"Memory usage after saving scaler: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nWarning: Training data is empty after preprocessing. Cannot scale features or train models.\")\n",
        "    X_train_scaled = X_train # Keep as is if empty\n",
        "    X_test_scaled = X_test   # Keep as is if empty\n",
        "    print(f\"Memory usage with empty data (scaling skipped): {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "# --- 3. & 4. Model Selection and Training ---\n",
        "\n",
        "# Initialize different classifiers\n",
        "models = {\n",
        "    # \"Logistic Regression\": LogisticRegression(max_iter=1000), # Increased max_iter for convergence\n",
        "    # \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    # \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    # \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5), # Default n_neighbors is 5\n",
        "    # \"Gaussian Naive Bayes\": GaussianNB(), # Added Naive Bayes\n",
        "    # \"SGD Classifier\": SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42), # Added SGD, using log_loss for logistic regression-like behavior\n",
        "    # \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42), # Added MLP\n",
        "    # \"Bagging Classifier (Decision Tree)\": BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42) # Fixed: Changed base_estimator to estimator\n",
        "    # \"Support Vector Machine (Linear)\": SVC(kernel='linear', random_state=42),\n",
        "    # \"Support Vector Machine (RBF)\": SVC(kernel='rbf', random_state=42),\n",
        "\n",
        "}\n",
        "\n",
        "# Add Gradient Boosting models if installed\n",
        "if XGBClassifier is not None:\n",
        "    models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42) # Added XGBoost\n",
        "if LGBMClassifier is not None:\n",
        "    models[\"LightGBM\"] = LGBMClassifier(random_state=42) # Added LightGBM\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\nTraining and evaluating models...\")\n",
        "\n",
        "# Only attempt training and evaluation if there is sufficient training and testing data\n",
        "if X_train_scaled.shape[0] > 0 and X_test_scaled.shape[0] > 0 and len(label_encoder.classes_) > 0:\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nMemory usage before training {name}: {get_memory_usage():.2f} MB\")\n",
        "        start_time = time.time()\n",
        "        print(f\"Training {name}...\")\n",
        "        try:\n",
        "            model.fit(X_train_scaled, y_train_encoded)\n",
        "            end_time = time.time()\n",
        "            training_time = end_time - start_time\n",
        "            print(f\"{name} training complete in {training_time:.4f} seconds.\")\n",
        "            print(f\"Memory usage after training {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "            # --- Save the trained model using pickle ---\n",
        "            try:\n",
        "                # Create a safe filename from the model name\n",
        "                filename = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\") + \".pkl\"\n",
        "                with open(filename, 'wb') as f:\n",
        "                    pickle.dump(model, f)\n",
        "                print(f\"Model '{name}' saved to {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving model '{name}': {e}\")\n",
        "            print(f\"Memory usage after saving model {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "            # --- 5. Evaluation ---\n",
        "            print(f\"\\nMemory usage before predicting with {name}: {get_memory_usage():.2f} MB\")\n",
        "            start_time = time.time()\n",
        "            y_pred_encoded = model.predict(X_test_scaled)\n",
        "            end_time = time.time()\n",
        "            prediction_time = end_time - start_time\n",
        "            print(f\"{name} prediction complete in {prediction_time:.4f} seconds.\")\n",
        "            print(f\"Memory usage after predicting with {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "            # Decode predictions back to original labels for clarity in report\n",
        "            # y_pred = label_encoder.inverse_transform(y_pred_encoded) # Decoding not strictly needed for metrics\n",
        "\n",
        "            accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "            try:\n",
        "                # Use classification_report for a comprehensive view\n",
        "                # zero_division=0 handles cases where a class has no true samples in the test set\n",
        "                report = classification_report(y_test_encoded, y_pred_encoded, target_names=label_encoder.classes_, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "                # Extract precision, recall, f1 from the report for easy printing\n",
        "                # This is a simplified way; parsing the report string might be needed for exact values\n",
        "                # For simplicity, we'll just print the full report.\n",
        "                # If you need specific average metrics (weighted, macro, micro), calculate them explicitly:\n",
        "                precision = precision_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                recall = recall_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                f1 = f1_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "\n",
        "\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Precision (Weighted)\": precision,\n",
        "                    \"Recall (Weighted)\": recall,\n",
        "                    \"F1-Score (Weighted)\": f1,\n",
        "                    \"Classification Report\": report,\n",
        "                    \"Confusion Matrix\": conf_matrix,\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "                print(f\"Memory usage after evaluating {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"Could not calculate all metrics for {name}. This might happen if test set has classes not in training set.\")\n",
        "                print(f\"Error details: {e}\")\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Classification Report\": \"Could not generate comprehensive report due to data limitations or missing classes in test set.\",\n",
        "                    \"Confusion Matrix\": \"Could not generate confusion matrix due to data limitations or missing classes in test set.\",\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
        "                print(f\"Memory usage after partial evaluation of {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training or evaluation of {name}: {e}\")\n",
        "            results[name] = {\n",
        "                \"Error\": str(e),\n",
        "                \"Training Time (s)\": training_time if 'training_time' in locals() else 'N/A',\n",
        "                \"Prediction Time (s)\": prediction_time if 'prediction_time' in locals() else 'N/A'\n",
        "            }\n",
        "            print(f\"Memory usage after error during {name} processing: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping model training and evaluation due to insufficient data or classes.\")\n",
        "    print(f\"Memory usage at the end of the script (training skipped): {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "# --- 6. Comparison ---\n",
        "\n",
        "print(\"\\n--- Model Comparison Results ---\")\n",
        "\n",
        "# Print results in a readable format\n",
        "if results:\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        if \"Error\" in metrics:\n",
        "            print(f\"Error during processing: {metrics['Error']}\")\n",
        "            print(f\"Training Time (s): {metrics.get('Training Time (s)', 'N/A'):.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics.get('Prediction Time (s)', 'N/A'):.4f}\")\n",
        "        else:\n",
        "            print(f\"Accuracy: {metrics.get('Accuracy', 'N/A'):.4f}\")\n",
        "            # Check if other metrics were calculated before printing\n",
        "            if 'Precision (Weighted)' in metrics:\n",
        "                print(f\"Precision (Weighted): {metrics['Precision (Weighted)']:.4f}\")\n",
        "                print(f\"Recall (Weighted): {metrics['Recall (Weighted)']:.4f}\")\n",
        "                print(f\"F1-Score (Weighted): {metrics['F1-Score (Weighted)']:.4f}\")\n",
        "            print(f\"Training Time (s): {metrics['Training Time (s)']:.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics['Prediction Time (s)']:.4f}\")\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(metrics['Classification Report'])\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(metrics['Confusion Matrix'])\n",
        "else:\n",
        "    print(\"No results to display. Model training and evaluation were skipped.\")\n",
        "\n",
        "print(f\"\\nFinal memory usage before script finishes: {get_memory_usage():.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TkwQoLHtGD0",
        "outputId": "58418c90-964a-4a14-acfb-4fc318026c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial memory usage: 239.12 MB\n",
            "\n",
            "Memory usage before loading data: 323.76 MB\n",
            "Training dataset loaded successfully from ./train_data.csv\n",
            "Training dataset shape: (2558780, 85)\n",
            "\n",
            "First 5 rows of the training dataset:\n",
            "                                   FlowID       SourceIP  SourcePort  \\\n",
            "0  192.168.10.3-192.168.10.12-53-26526-17  192.168.10.12       26526   \n",
            "1   172.16.0.1-192.168.10.50-37255-3737-6     172.16.0.1       37255   \n",
            "2    192.168.10.16-72.21.91.29-53482-80-6  192.168.10.16       53482   \n",
            "3    192.168.10.15-31.13.71.7-50902-443-6     31.13.71.7         443   \n",
            "4   192.168.10.3-192.168.10.9-53-51576-17   192.168.10.9       51576   \n",
            "\n",
            "   DestinationIP  DestinationPort  Protocol            Timestamp  \\\n",
            "0   192.168.10.3               53        17        6/7/2017 3:12   \n",
            "1  192.168.10.50             3737         6        7/7/2017 2:52   \n",
            "2    72.21.91.29               80         6       5/7/2017 10:02   \n",
            "3  192.168.10.15            50902         6  03/07/2017 10:16:29   \n",
            "4   192.168.10.3               53        17        4/7/2017 4:59   \n",
            "\n",
            "   FlowDuration  TotalFwdPackets  TotalBackwardPackets  ...  \\\n",
            "0         31419                2                     2  ...   \n",
            "1            33                1                     1  ...   \n",
            "2       5713462                3                     1  ...   \n",
            "3           240                3                     1  ...   \n",
            "4           187                2                     2  ...   \n",
            "\n",
            "   min_seg_size_forward  ActiveMean  ActiveStd  ActiveMax  ActiveMin  \\\n",
            "0                    20         0.0        0.0        0.0        0.0   \n",
            "1                    24         0.0        0.0        0.0        0.0   \n",
            "2                    32         0.0        0.0        0.0        0.0   \n",
            "3                    20         0.0        0.0        0.0        0.0   \n",
            "4                    20         0.0        0.0        0.0        0.0   \n",
            "\n",
            "   IdleMean  IdleStd  IdleMax  IdleMin     Label  \n",
            "0       0.0      0.0      0.0      0.0    BENIGN  \n",
            "1       0.0      0.0      0.0      0.0  PortScan  \n",
            "2       0.0      0.0      0.0      0.0    BENIGN  \n",
            "3       0.0      0.0      0.0      0.0    BENIGN  \n",
            "4       0.0      0.0      0.0      0.0    BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Training Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2558780 entries, 0 to 2558779\n",
            "Data columns (total 85 columns):\n",
            " #   Column                   Dtype  \n",
            "---  ------                   -----  \n",
            " 0   FlowID                   object \n",
            " 1   SourceIP                 object \n",
            " 2   SourcePort               int64  \n",
            " 3   DestinationIP            object \n",
            " 4   DestinationPort          int64  \n",
            " 5   Protocol                 int64  \n",
            " 6   Timestamp                object \n",
            " 7   FlowDuration             int64  \n",
            " 8   TotalFwdPackets          int64  \n",
            " 9   TotalBackwardPackets     int64  \n",
            " 10  TotalLengthofFwdPackets  float64\n",
            " 11  TotalLengthofBwdPackets  float64\n",
            " 12  FwdPacketLengthMax       float64\n",
            " 13  FwdPacketLengthMin       float64\n",
            " 14  FwdPacketLengthMean      float64\n",
            " 15  FwdPacketLengthStd       float64\n",
            " 16  BwdPacketLengthMax       float64\n",
            " 17  BwdPacketLengthMin       float64\n",
            " 18  BwdPacketLengthMean      float64\n",
            " 19  BwdPacketLengthStd       float64\n",
            " 20  FlowBytes/s              float64\n",
            " 21  FlowPackets/s            float64\n",
            " 22  FlowIATMean              float64\n",
            " 23  FlowIATStd               float64\n",
            " 24  FlowIATMax               float64\n",
            " 25  FlowIATMin               float64\n",
            " 26  FwdIATTotal              float64\n",
            " 27  FwdIATMean               float64\n",
            " 28  FwdIATStd                float64\n",
            " 29  FwdIATMax                float64\n",
            " 30  FwdIATMin                float64\n",
            " 31  BwdIATTotal              float64\n",
            " 32  BwdIATMean               float64\n",
            " 33  BwdIATStd                float64\n",
            " 34  BwdIATMax                float64\n",
            " 35  BwdIATMin                float64\n",
            " 36  FwdPSHFlags              int64  \n",
            " 37  BwdPSHFlags              int64  \n",
            " 38  FwdURGFlags              int64  \n",
            " 39  BwdURGFlags              int64  \n",
            " 40  FwdHeaderLength          int64  \n",
            " 41  BwdHeaderLength          int64  \n",
            " 42  FwdPackets/s             float64\n",
            " 43  BwdPackets/s             float64\n",
            " 44  MinPacketLength          float64\n",
            " 45  MaxPacketLength          float64\n",
            " 46  PacketLengthMean         float64\n",
            " 47  PacketLengthStd          float64\n",
            " 48  PacketLengthVariance     float64\n",
            " 49  FINFlagCount             int64  \n",
            " 50  SYNFlagCount             int64  \n",
            " 51  RSTFlagCount             int64  \n",
            " 52  PSHFlagCount             int64  \n",
            " 53  ACKFlagCount             int64  \n",
            " 54  URGFlagCount             int64  \n",
            " 55  CWEFlagCount             int64  \n",
            " 56  ECEFlagCount             int64  \n",
            " 57  Down/UpRatio             float64\n",
            " 58  AveragePacketSize        float64\n",
            " 59  AvgFwdSegmentSize        float64\n",
            " 60  AvgBwdSegmentSize        float64\n",
            " 61  FwdHeaderLength.1        int64  \n",
            " 62  FwdAvgBytes/Bulk         int64  \n",
            " 63  FwdAvgPackets/Bulk       int64  \n",
            " 64  FwdAvgBulkRate           int64  \n",
            " 65  BwdAvgBytes/Bulk         int64  \n",
            " 66  BwdAvgPackets/Bulk       int64  \n",
            " 67  BwdAvgBulkRate           int64  \n",
            " 68  SubflowFwdPackets        int64  \n",
            " 69  SubflowFwdBytes          int64  \n",
            " 70  SubflowBwdPackets        int64  \n",
            " 71  SubflowBwdBytes          int64  \n",
            " 72  Init_Win_bytes_forward   int64  \n",
            " 73  Init_Win_bytes_backward  int64  \n",
            " 74  act_data_pkt_fwd         int64  \n",
            " 75  min_seg_size_forward     int64  \n",
            " 76  ActiveMean               float64\n",
            " 77  ActiveStd                float64\n",
            " 78  ActiveMax                float64\n",
            " 79  ActiveMin                float64\n",
            " 80  IdleMean                 float64\n",
            " 81  IdleStd                  float64\n",
            " 82  IdleMax                  float64\n",
            " 83  IdleMin                  float64\n",
            " 84  Label                    object \n",
            "dtypes: float64(45), int64(35), object(5)\n",
            "memory usage: 1.6+ GB\n",
            "Memory usage after loading training data: 3875.80 MB\n",
            "\n",
            "Testing dataset loaded successfully from ./test_data.csv\n",
            "Testing dataset shape: (639695, 85)\n",
            "\n",
            "First 5 rows of the testing dataset:\n",
            "                                    FlowID       SourceIP  SourcePort  \\\n",
            "0   192.168.10.5-211.233.74.132-58565-80-6   192.168.10.5       58565   \n",
            "1   192.168.10.3-192.168.10.14-53-52520-17  192.168.10.14       52520   \n",
            "2      172.16.0.1-192.168.10.50-39234-80-6     172.16.0.1       39234   \n",
            "3  192.168.10.25-104.97.133.94-55588-443-6  192.168.10.25       55588   \n",
            "4  157.240.18.35-192.168.10.15-443-54074-6  192.168.10.15       54074   \n",
            "\n",
            "    DestinationIP  DestinationPort  Protocol            Timestamp  \\\n",
            "0  211.233.74.132               80         6        6/7/2017 4:55   \n",
            "1    192.168.10.3               53        17        7/7/2017 1:57   \n",
            "2   192.168.10.50               80         6       5/7/2017 10:45   \n",
            "3   104.97.133.94              443         6  03/07/2017 10:55:48   \n",
            "4   157.240.18.35              443         6       4/7/2017 11:47   \n",
            "\n",
            "   FlowDuration  TotalFwdPackets  TotalBackwardPackets  ...  \\\n",
            "0       5658973                3                     1  ...   \n",
            "1           346                2                     2  ...   \n",
            "2             1                2                     0  ...   \n",
            "3         77778               11                     6  ...   \n",
            "4          5630                1                     1  ...   \n",
            "\n",
            "   min_seg_size_forward  ActiveMean  ActiveStd  ActiveMax  ActiveMin  \\\n",
            "0                    20         0.0        0.0        0.0        0.0   \n",
            "1                    20         0.0        0.0        0.0        0.0   \n",
            "2                    32         0.0        0.0        0.0        0.0   \n",
            "3                    32         0.0        0.0        0.0        0.0   \n",
            "4                    20         0.0        0.0        0.0        0.0   \n",
            "\n",
            "   IdleMean  IdleStd  IdleMax  IdleMin     Label  \n",
            "0       0.0      0.0      0.0      0.0    BENIGN  \n",
            "1       0.0      0.0      0.0      0.0    BENIGN  \n",
            "2       0.0      0.0      0.0      0.0  DoS Hulk  \n",
            "3       0.0      0.0      0.0      0.0    BENIGN  \n",
            "4       0.0      0.0      0.0      0.0    BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Testing Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 639695 entries, 0 to 639694\n",
            "Data columns (total 85 columns):\n",
            " #   Column                   Non-Null Count   Dtype  \n",
            "---  ------                   --------------   -----  \n",
            " 0   FlowID                   639695 non-null  object \n",
            " 1   SourceIP                 639695 non-null  object \n",
            " 2   SourcePort               639695 non-null  int64  \n",
            " 3   DestinationIP            639695 non-null  object \n",
            " 4   DestinationPort          639695 non-null  int64  \n",
            " 5   Protocol                 639695 non-null  int64  \n",
            " 6   Timestamp                639695 non-null  object \n",
            " 7   FlowDuration             639695 non-null  int64  \n",
            " 8   TotalFwdPackets          639695 non-null  int64  \n",
            " 9   TotalBackwardPackets     639695 non-null  int64  \n",
            " 10  TotalLengthofFwdPackets  639695 non-null  float64\n",
            " 11  TotalLengthofBwdPackets  639695 non-null  float64\n",
            " 12  FwdPacketLengthMax       639695 non-null  float64\n",
            " 13  FwdPacketLengthMin       639695 non-null  float64\n",
            " 14  FwdPacketLengthMean      639695 non-null  float64\n",
            " 15  FwdPacketLengthStd       639695 non-null  float64\n",
            " 16  BwdPacketLengthMax       639695 non-null  float64\n",
            " 17  BwdPacketLengthMin       639695 non-null  float64\n",
            " 18  BwdPacketLengthMean      639695 non-null  float64\n",
            " 19  BwdPacketLengthStd       639695 non-null  float64\n",
            " 20  FlowBytes/s              639695 non-null  float64\n",
            " 21  FlowPackets/s            639695 non-null  float64\n",
            " 22  FlowIATMean              639695 non-null  float64\n",
            " 23  FlowIATStd               639695 non-null  float64\n",
            " 24  FlowIATMax               639695 non-null  float64\n",
            " 25  FlowIATMin               639695 non-null  float64\n",
            " 26  FwdIATTotal              639695 non-null  float64\n",
            " 27  FwdIATMean               639695 non-null  float64\n",
            " 28  FwdIATStd                639695 non-null  float64\n",
            " 29  FwdIATMax                639695 non-null  float64\n",
            " 30  FwdIATMin                639695 non-null  float64\n",
            " 31  BwdIATTotal              639695 non-null  float64\n",
            " 32  BwdIATMean               639695 non-null  float64\n",
            " 33  BwdIATStd                639695 non-null  float64\n",
            " 34  BwdIATMax                639695 non-null  float64\n",
            " 35  BwdIATMin                639695 non-null  float64\n",
            " 36  FwdPSHFlags              639695 non-null  int64  \n",
            " 37  BwdPSHFlags              639695 non-null  int64  \n",
            " 38  FwdURGFlags              639695 non-null  int64  \n",
            " 39  BwdURGFlags              639695 non-null  int64  \n",
            " 40  FwdHeaderLength          639695 non-null  int64  \n",
            " 41  BwdHeaderLength          639695 non-null  int64  \n",
            " 42  FwdPackets/s             639695 non-null  float64\n",
            " 43  BwdPackets/s             639695 non-null  float64\n",
            " 44  MinPacketLength          639695 non-null  float64\n",
            " 45  MaxPacketLength          639695 non-null  float64\n",
            " 46  PacketLengthMean         639695 non-null  float64\n",
            " 47  PacketLengthStd          639695 non-null  float64\n",
            " 48  PacketLengthVariance     639695 non-null  float64\n",
            " 49  FINFlagCount             639695 non-null  int64  \n",
            " 50  SYNFlagCount             639695 non-null  int64  \n",
            " 51  RSTFlagCount             639695 non-null  int64  \n",
            " 52  PSHFlagCount             639695 non-null  int64  \n",
            " 53  ACKFlagCount             639695 non-null  int64  \n",
            " 54  URGFlagCount             639695 non-null  int64  \n",
            " 55  CWEFlagCount             639695 non-null  int64  \n",
            " 56  ECEFlagCount             639695 non-null  int64  \n",
            " 57  Down/UpRatio             639695 non-null  float64\n",
            " 58  AveragePacketSize        639695 non-null  float64\n",
            " 59  AvgFwdSegmentSize        639695 non-null  float64\n",
            " 60  AvgBwdSegmentSize        639695 non-null  float64\n",
            " 61  FwdHeaderLength.1        639695 non-null  int64  \n",
            " 62  FwdAvgBytes/Bulk         639695 non-null  int64  \n",
            " 63  FwdAvgPackets/Bulk       639695 non-null  int64  \n",
            " 64  FwdAvgBulkRate           639695 non-null  int64  \n",
            " 65  BwdAvgBytes/Bulk         639695 non-null  int64  \n",
            " 66  BwdAvgPackets/Bulk       639695 non-null  int64  \n",
            " 67  BwdAvgBulkRate           639695 non-null  int64  \n",
            " 68  SubflowFwdPackets        639695 non-null  int64  \n",
            " 69  SubflowFwdBytes          639695 non-null  int64  \n",
            " 70  SubflowBwdPackets        639695 non-null  int64  \n",
            " 71  SubflowBwdBytes          639695 non-null  int64  \n",
            " 72  Init_Win_bytes_forward   639695 non-null  int64  \n",
            " 73  Init_Win_bytes_backward  639695 non-null  int64  \n",
            " 74  act_data_pkt_fwd         639695 non-null  int64  \n",
            " 75  min_seg_size_forward     639695 non-null  int64  \n",
            " 76  ActiveMean               639695 non-null  float64\n",
            " 77  ActiveStd                639695 non-null  float64\n",
            " 78  ActiveMax                639695 non-null  float64\n",
            " 79  ActiveMin                639695 non-null  float64\n",
            " 80  IdleMean                 639695 non-null  float64\n",
            " 81  IdleStd                  639695 non-null  float64\n",
            " 82  IdleMax                  639695 non-null  float64\n",
            " 83  IdleMin                  639695 non-null  float64\n",
            " 84  Label                    639695 non-null  object \n",
            "dtypes: float64(45), int64(35), object(5)\n",
            "memory usage: 414.8+ MB\n",
            "Memory usage after loading testing data: 4175.24 MB\n",
            "\n",
            "Memory usage before preprocessing: 4175.24 MB\n",
            "\n",
            "Preprocessing applied to both training and testing datasets.\n",
            "Memory usage after initial preprocessing: 5956.69 MB\n",
            "\n",
            "Labels simplified to 'BENIGN' vs 'ATTACK'.\n",
            "Memory usage after simplifying labels: 5956.69 MB\n",
            "Encoded value 0 -> Class label 'ATTACK'\n",
            "Encoded value 1 -> Class label 'BENIGN'\n",
            "\n",
            "Original Training Labels: ['BENIGN' 'ATTACK']\n",
            "Encoded Training Labels Classes: ['ATTACK' 'BENIGN']\n",
            "Encoded Training Labels: LabelEncoder()\n",
            "\n",
            "Original Testing Labels: ['BENIGN' 'ATTACK']\n",
            "Encoded Testing Labels (based on training labels): ['ATTACK' 'BENIGN']\n",
            "\n",
            "Label encoder saved to label_encoder.pkl\n",
            "Memory usage after encoding labels and saving encoder: 5956.69 MB\n",
            "\n",
            "Memory usage before scaling features: 5956.69 MB\n",
            "Features scaled using StandardScaler (fitted on training data).\n",
            "Memory usage after scaling features: 7902.44 MB\n",
            "Standard scaler saved to scaler.pkl\n",
            "Memory usage after saving scaler: 7902.44 MB\n",
            "\n",
            "Training and evaluating models...\n",
            "\n",
            "Memory usage before training XGBoost: 7902.44 MB\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:28:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost training complete in 95.1934 seconds.\n",
            "Memory usage after training XGBoost: 7927.92 MB\n",
            "Model 'XGBoost' saved to XGBoost.pkl\n",
            "Memory usage after saving model XGBoost: 7927.92 MB\n",
            "\n",
            "Memory usage before predicting with XGBoost: 7927.92 MB\n",
            "XGBoost prediction complete in 1.9096 seconds.\n",
            "Memory usage after predicting with XGBoost: 7927.92 MB\n",
            "XGBoost - Accuracy: 0.9999, Precision: 0.9999, Recall: 0.9999, F1-Score: 0.9999\n",
            "Memory usage after evaluating XGBoost: 7927.92 MB\n",
            "\n",
            "Memory usage before training LightGBM: 7927.92 MB\n",
            "Training LightGBM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1979150, number of negative: 579630\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.584952 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 14858\n",
            "[LightGBM] [Info] Number of data points in the train set: 2558780, number of used features: 72\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.773474 -> initscore=1.228033\n",
            "[LightGBM] [Info] Start training from score 1.228033\n",
            "LightGBM training complete in 120.7362 seconds.\n",
            "Memory usage after training LightGBM: 7888.56 MB\n",
            "Model 'LightGBM' saved to LightGBM.pkl\n",
            "Memory usage after saving model LightGBM: 7888.56 MB\n",
            "\n",
            "Memory usage before predicting with LightGBM: 7888.56 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM prediction complete in 4.1356 seconds.\n",
            "Memory usage after predicting with LightGBM: 7889.56 MB\n",
            "LightGBM - Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9997, F1-Score: 0.9997\n",
            "Memory usage after evaluating LightGBM: 7889.56 MB\n",
            "\n",
            "--- Model Comparison Results ---\n",
            "\n",
            "--- XGBoost ---\n",
            "Accuracy: 0.9999\n",
            "Precision (Weighted): 0.9999\n",
            "Recall (Weighted): 0.9999\n",
            "F1-Score (Weighted): 0.9999\n",
            "Training Time (s): 95.1934\n",
            "Prediction Time (s): 1.9096\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       1.00      1.00      1.00    145426\n",
            "      BENIGN       1.00      1.00      1.00    494269\n",
            "\n",
            "    accuracy                           1.00    639695\n",
            "   macro avg       1.00      1.00      1.00    639695\n",
            "weighted avg       1.00      1.00      1.00    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145401     25]\n",
            " [    49 494220]]\n",
            "\n",
            "--- LightGBM ---\n",
            "Accuracy: 0.9997\n",
            "Precision (Weighted): 0.9997\n",
            "Recall (Weighted): 0.9997\n",
            "F1-Score (Weighted): 0.9997\n",
            "Training Time (s): 120.7362\n",
            "Prediction Time (s): 4.1356\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       1.00      1.00      1.00    145426\n",
            "      BENIGN       1.00      1.00      1.00    494269\n",
            "\n",
            "    accuracy                           1.00    639695\n",
            "   macro avg       1.00      1.00      1.00    639695\n",
            "weighted avg       1.00      1.00      1.00    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145386     40]\n",
            " [   146 494123]]\n",
            "\n",
            "Final memory usage before script finishes: 7889.56 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Working\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split # Still useful for potential validation split later, but not for initial train/test load\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier # Added SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier # Added BaggingClassifier (as an example of bagging)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB # Added Gaussian Naive Bayes\n",
        "from sklearn.neural_network import MLPClassifier # Added MLP Classifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import time\n",
        "import io # Keep io import in case needed for other things\n",
        "import pickle # Import the pickle library\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "\n",
        "# Import Gradient Boosting Libraries (you might need to install these: pip install xgboost lightgbm)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBClassifier = xgb.XGBClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: XGBoost not installed. Skipping XGBoost model.\")\n",
        "    XGBClassifier = None # Set to None if not installed\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGBMClassifier = lgb.LGBMClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: LightGBM not installed. Skipping LightGBM model.\")\n",
        "    LGBMClassifier = None # Set to None if not installed\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace with the actual paths to your dataset files\n",
        "TRAIN_DATASET_PATH = './train_data.csv' # <--- Path to your training CSV file\n",
        "TEST_DATASET_PATH = './test_data.csv'   # <--- Path to your testing CSV file\n",
        "TARGET_COLUMN = 'Label' # The name of the target column\n",
        "\n",
        "# --- 1. Load Data from Separate CSV Files ---\n",
        "try:\n",
        "    df_train = pd.read_csv(TRAIN_DATASET_PATH)\n",
        "    print(f\"Training dataset loaded successfully from {TRAIN_DATASET_PATH}\")\n",
        "    print(f\"Training dataset shape: {df_train.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the training dataset:\")\n",
        "    print(df_train.head())\n",
        "    print(\"\\nTraining Dataset Info:\")\n",
        "    df_train.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Training dataset not found at {TRAIN_DATASET_PATH}\")\n",
        "    print(\"Please update TRAIN_DATASET_PATH with the correct path to your training CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "try:\n",
        "    df_test = pd.read_csv(TEST_DATASET_PATH)\n",
        "    print(f\"\\nTesting dataset loaded successfully from {TEST_DATASET_PATH}\")\n",
        "    print(f\"Testing dataset shape: {df_test.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the testing dataset:\")\n",
        "    print(df_test.head())\n",
        "    print(\"\\nTesting Dataset Info:\")\n",
        "    df_test.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Testing dataset not found at {TEST_DATASET_PATH}\")\n",
        "    print(\"Please update TEST_DATASET_PATH with the correct path to your testing CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "\n",
        "# --- 2. Preprocessing ---\n",
        "\n",
        "# Define columns to drop - apply to both train and test sets\n",
        "columns_to_drop = ['FlowID', 'SourceIP', 'DestinationIP', 'Timestamp']\n",
        "\n",
        "# Apply dropping columns and handling infinite/NaN values to both dataframes\n",
        "def preprocess_dataframe(df, columns_to_drop, target_column):\n",
        "    df_processed = df.drop(columns=columns_to_drop, errors='ignore').copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    # Handle potential infinite values\n",
        "    df_processed.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
        "    # Fill NaN values. Using 0 here, but consider other strategies.\n",
        "    df_processed.fillna(0, inplace=True)\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    X = df_processed.drop(columns=[target_column])\n",
        "    y = df_processed[target_column]\n",
        "\n",
        "    # --- Added: Ensure target column is treated as string for LabelEncoder ---\n",
        "    y = y.astype(str)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_dataframe(df_train, columns_to_drop, TARGET_COLUMN)\n",
        "X_test, y_test = preprocess_dataframe(df_test, columns_to_drop, TARGET_COLUMN)\n",
        "\n",
        "# --- NEW: Convert multi-class target into binary labels: 'BENIGN' vs 'ATTACK' ---\n",
        "def simplify_labels(y):\n",
        "    return y.apply(lambda x: 'BENIGN' if x.upper() == 'BENIGN' else 'ATTACK')\n",
        "\n",
        "y_train = simplify_labels(y_train)\n",
        "y_test = simplify_labels(y_test)\n",
        "\n",
        "print(\"\\nPreprocessing applied to both training and testing datasets.\")\n",
        "\n",
        "# Encode the target variable if it's categorical (e.g., 'BENIGN', 'ATTACK_TYPE')\n",
        "# Fit on training labels, transform both training and testing labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test) # Use transform, not fit_transform on test set\n",
        "\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"Encoded value {i} -> Class label '{class_name}'\")\n",
        "\n",
        "print(f\"\\nOriginal Training Labels: {y_train.unique()}\")\n",
        "print(f\"Encoded Training Labels Classes: {label_encoder.classes_}\")\n",
        "print(f\"Encoded Training Labels: {label_encoder}\")\n",
        "print(f\"\\nOriginal Testing Labels: {y_test.unique()}\")\n",
        "print(f\"Encoded Testing Labels (based on training labels): {label_encoder.classes_}\")\n",
        "\n",
        "# --- Save the LabelEncoder ---\n",
        "try:\n",
        "    encoder_filename = 'label_encoder.pkl'\n",
        "    with open(encoder_filename, 'wb') as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "    print(f\"\\nLabel encoder saved to {encoder_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving label encoder: {e}\")\n",
        "\n",
        "\n",
        "# Scale numerical features\n",
        "# Fit scaler ONLY on training data, then transform both train and test data\n",
        "scaler = StandardScaler()\n",
        "# Check if training data is not empty before scaling\n",
        "if X_train.shape[0] > 0:\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    print(f\"\\nFeatures scaled using StandardScaler (fitted on training data).\")\n",
        "\n",
        "    # --- Save the StandardScaler ---\n",
        "    try:\n",
        "        scaler_filename = 'scaler.pkl'\n",
        "        with open(scaler_filename, 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        print(f\"Standard scaler saved to {scaler_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving standard scaler: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nWarning: Training data is empty after preprocessing. Cannot scale features or train models.\")\n",
        "    X_train_scaled = X_train # Keep as is if empty\n",
        "    X_test_scaled = X_test   # Keep as is if empty\n",
        "\n",
        "\n",
        "# --- 3. & 4. Model Selection and Training ---\n",
        "\n",
        "# Initialize different classifiers\n",
        "models = {\n",
        "    # \"Logistic Regression\": LogisticRegression(max_iter=1000), # Increased max_iter for convergence\n",
        "    # \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    # \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    # \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5), # Default n_neighbors is 5\n",
        "    # \"Gaussian Naive Bayes\": GaussianNB(), # Added Naive Bayes\n",
        "    # \"SGD Classifier\": SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42), # Added SGD, using log_loss for logistic regression-like behavior\n",
        "    # \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42), # Added MLP\n",
        "    # \"Bagging Classifier (Decision Tree)\": BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42) # Fixed: Changed base_estimator to estimator\n",
        "    # \"Support Vector Machine (Linear)\": SVC(kernel='linear', random_state=42),\n",
        "    # \"Support Vector Machine (RBF)\": SVC(kernel='rbf', random_state=42),\n",
        "\n",
        "}\n",
        "\n",
        "# Add Gradient Boosting models if installed\n",
        "if XGBClassifier is not None:\n",
        "    models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42) # Added XGBoost\n",
        "if LGBMClassifier is not None:\n",
        "    models[\"LightGBM\"] = LGBMClassifier(random_state=42) # Added LightGBM\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\nTraining and evaluating models...\")\n",
        "\n",
        "# Only attempt training and evaluation if there is sufficient training and testing data\n",
        "if X_train_scaled.shape[0] > 0 and X_test_scaled.shape[0] > 0 and len(label_encoder.classes_) > 0:\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        try:\n",
        "            model.fit(X_train_scaled, y_train_encoded)\n",
        "            end_time = time.time()\n",
        "            training_time = end_time - start_time\n",
        "            print(f\"{name} training complete in {training_time:.4f} seconds.\")\n",
        "\n",
        "            # --- Save the trained model using pickle ---\n",
        "            try:\n",
        "                # Create a safe filename from the model name\n",
        "                filename = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\") + \".pkl\"\n",
        "                with open(filename, 'wb') as f:\n",
        "                    pickle.dump(model, f)\n",
        "                print(f\"Model '{name}' saved to {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving model '{name}': {e}\")\n",
        "\n",
        "\n",
        "            # --- 5. Evaluation ---\n",
        "            start_time = time.time()\n",
        "            y_pred_encoded = model.predict(X_test_scaled)\n",
        "            end_time = time.time()\n",
        "            prediction_time = end_time - start_time\n",
        "            print(f\"{name} prediction complete in {prediction_time:.4f} seconds.\")\n",
        "\n",
        "            # Decode predictions back to original labels for clarity in report\n",
        "            # y_pred = label_encoder.inverse_transform(y_pred_encoded) # Decoding not strictly needed for metrics\n",
        "\n",
        "            accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "            try:\n",
        "                # Use classification_report for a comprehensive view\n",
        "                # zero_division=0 handles cases where a class has no true samples in the test set\n",
        "                report = classification_report(y_test_encoded, y_pred_encoded, target_names=label_encoder.classes_, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "                # Extract precision, recall, f1 from the report for easy printing\n",
        "                # This is a simplified way; parsing the report string might be needed for exact values\n",
        "                # For simplicity, we'll just print the full report.\n",
        "                # If you need specific average metrics (weighted, macro, micro), calculate them explicitly:\n",
        "                precision = precision_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                recall = recall_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                f1 = f1_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "\n",
        "\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Precision (Weighted)\": precision,\n",
        "                    \"Recall (Weighted)\": recall,\n",
        "                    \"F1-Score (Weighted)\": f1,\n",
        "                    \"Classification Report\": report,\n",
        "                    \"Confusion Matrix\": conf_matrix,\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "            except ValueError as e:\n",
        "                print(f\"Could not calculate all metrics for {name}. This might happen if test set has classes not in training set.\")\n",
        "                print(f\"Error details: {e}\")\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Classification Report\": \"Could not generate comprehensive report due to data limitations or missing classes in test set.\",\n",
        "                    \"Confusion Matrix\": \"Could not generate confusion matrix due to data limitations or missing classes in test set.\",\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training or evaluation of {name}: {e}\")\n",
        "            results[name] = {\n",
        "                \"Error\": str(e),\n",
        "                \"Training Time (s)\": training_time if 'training_time' in locals() else 'N/A',\n",
        "                \"Prediction Time (s)\": prediction_time if 'prediction_time' in locals() else 'N/A'\n",
        "            }\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping model training and evaluation due to insufficient data or classes.\")\n",
        "\n",
        "\n",
        "# --- 6. Comparison ---\n",
        "\n",
        "print(\"\\n--- Model Comparison Results ---\")\n",
        "\n",
        "# Print results in a readable format\n",
        "if results:\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        if \"Error\" in metrics:\n",
        "            print(f\"Error during processing: {metrics['Error']}\")\n",
        "            print(f\"Training Time (s): {metrics.get('Training Time (s)', 'N/A'):.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics.get('Prediction Time (s)', 'N/A'):.4f}\")\n",
        "        else:\n",
        "            print(f\"Accuracy: {metrics.get('Accuracy', 'N/A'):.4f}\")\n",
        "            # Check if other metrics were calculated before printing\n",
        "            if 'Precision (Weighted)' in metrics:\n",
        "                print(f\"Precision (Weighted): {metrics['Precision (Weighted)']:.4f}\")\n",
        "                print(f\"Recall (Weighted): {metrics['Recall (Weighted)']:.4f}\")\n",
        "                print(f\"F1-Score (Weighted): {metrics['F1-Score (Weighted)']:.4f}\")\n",
        "            print(f\"Training Time (s): {metrics['Training Time (s)']:.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics['Prediction Time (s)']:.4f}\")\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(metrics['Classification Report'])\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(metrics['Confusion Matrix'])\n",
        "else:\n",
        "    print(\"No results to display. Model training and evaluation were skipped.\")"
      ],
      "metadata": {
        "id": "AIKzIs1Y9tWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06b4d0e-3420-41f0-eb5b-1b56581a908d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset loaded successfully from ./train_data.csv\n",
            "Training dataset shape: (2558780, 85)\n",
            "\n",
            "First 5 rows of the training dataset:\n",
            "                                   FlowID       SourceIP  SourcePort  \\\n",
            "0  192.168.10.3-192.168.10.12-53-26526-17  192.168.10.12       26526   \n",
            "1   172.16.0.1-192.168.10.50-37255-3737-6     172.16.0.1       37255   \n",
            "2    192.168.10.16-72.21.91.29-53482-80-6  192.168.10.16       53482   \n",
            "3    192.168.10.15-31.13.71.7-50902-443-6     31.13.71.7         443   \n",
            "4   192.168.10.3-192.168.10.9-53-51576-17   192.168.10.9       51576   \n",
            "\n",
            "   DestinationIP  DestinationPort  Protocol            Timestamp  \\\n",
            "0   192.168.10.3               53        17        6/7/2017 3:12   \n",
            "1  192.168.10.50             3737         6        7/7/2017 2:52   \n",
            "2    72.21.91.29               80         6       5/7/2017 10:02   \n",
            "3  192.168.10.15            50902         6  03/07/2017 10:16:29   \n",
            "4   192.168.10.3               53        17        4/7/2017 4:59   \n",
            "\n",
            "   FlowDuration  TotalFwdPackets  TotalBackwardPackets  ...  \\\n",
            "0         31419                2                     2  ...   \n",
            "1            33                1                     1  ...   \n",
            "2       5713462                3                     1  ...   \n",
            "3           240                3                     1  ...   \n",
            "4           187                2                     2  ...   \n",
            "\n",
            "   min_seg_size_forward  ActiveMean  ActiveStd  ActiveMax  ActiveMin  \\\n",
            "0                    20         0.0        0.0        0.0        0.0   \n",
            "1                    24         0.0        0.0        0.0        0.0   \n",
            "2                    32         0.0        0.0        0.0        0.0   \n",
            "3                    20         0.0        0.0        0.0        0.0   \n",
            "4                    20         0.0        0.0        0.0        0.0   \n",
            "\n",
            "   IdleMean  IdleStd  IdleMax  IdleMin     Label  \n",
            "0       0.0      0.0      0.0      0.0    BENIGN  \n",
            "1       0.0      0.0      0.0      0.0  PortScan  \n",
            "2       0.0      0.0      0.0      0.0    BENIGN  \n",
            "3       0.0      0.0      0.0      0.0    BENIGN  \n",
            "4       0.0      0.0      0.0      0.0    BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Training Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2558780 entries, 0 to 2558779\n",
            "Data columns (total 85 columns):\n",
            " #   Column                   Dtype  \n",
            "---  ------                   -----  \n",
            " 0   FlowID                   object \n",
            " 1   SourceIP                 object \n",
            " 2   SourcePort               int64  \n",
            " 3   DestinationIP            object \n",
            " 4   DestinationPort          int64  \n",
            " 5   Protocol                 int64  \n",
            " 6   Timestamp                object \n",
            " 7   FlowDuration             int64  \n",
            " 8   TotalFwdPackets          int64  \n",
            " 9   TotalBackwardPackets     int64  \n",
            " 10  TotalLengthofFwdPackets  float64\n",
            " 11  TotalLengthofBwdPackets  float64\n",
            " 12  FwdPacketLengthMax       float64\n",
            " 13  FwdPacketLengthMin       float64\n",
            " 14  FwdPacketLengthMean      float64\n",
            " 15  FwdPacketLengthStd       float64\n",
            " 16  BwdPacketLengthMax       float64\n",
            " 17  BwdPacketLengthMin       float64\n",
            " 18  BwdPacketLengthMean      float64\n",
            " 19  BwdPacketLengthStd       float64\n",
            " 20  FlowBytes/s              float64\n",
            " 21  FlowPackets/s            float64\n",
            " 22  FlowIATMean              float64\n",
            " 23  FlowIATStd               float64\n",
            " 24  FlowIATMax               float64\n",
            " 25  FlowIATMin               float64\n",
            " 26  FwdIATTotal              float64\n",
            " 27  FwdIATMean               float64\n",
            " 28  FwdIATStd                float64\n",
            " 29  FwdIATMax                float64\n",
            " 30  FwdIATMin                float64\n",
            " 31  BwdIATTotal              float64\n",
            " 32  BwdIATMean               float64\n",
            " 33  BwdIATStd                float64\n",
            " 34  BwdIATMax                float64\n",
            " 35  BwdIATMin                float64\n",
            " 36  FwdPSHFlags              int64  \n",
            " 37  BwdPSHFlags              int64  \n",
            " 38  FwdURGFlags              int64  \n",
            " 39  BwdURGFlags              int64  \n",
            " 40  FwdHeaderLength          int64  \n",
            " 41  BwdHeaderLength          int64  \n",
            " 42  FwdPackets/s             float64\n",
            " 43  BwdPackets/s             float64\n",
            " 44  MinPacketLength          float64\n",
            " 45  MaxPacketLength          float64\n",
            " 46  PacketLengthMean         float64\n",
            " 47  PacketLengthStd          float64\n",
            " 48  PacketLengthVariance     float64\n",
            " 49  FINFlagCount             int64  \n",
            " 50  SYNFlagCount             int64  \n",
            " 51  RSTFlagCount             int64  \n",
            " 52  PSHFlagCount             int64  \n",
            " 53  ACKFlagCount             int64  \n",
            " 54  URGFlagCount             int64  \n",
            " 55  CWEFlagCount             int64  \n",
            " 56  ECEFlagCount             int64  \n",
            " 57  Down/UpRatio             float64\n",
            " 58  AveragePacketSize        float64\n",
            " 59  AvgFwdSegmentSize        float64\n",
            " 60  AvgBwdSegmentSize        float64\n",
            " 61  FwdHeaderLength.1        int64  \n",
            " 62  FwdAvgBytes/Bulk         int64  \n",
            " 63  FwdAvgPackets/Bulk       int64  \n",
            " 64  FwdAvgBulkRate           int64  \n",
            " 65  BwdAvgBytes/Bulk         int64  \n",
            " 66  BwdAvgPackets/Bulk       int64  \n",
            " 67  BwdAvgBulkRate           int64  \n",
            " 68  SubflowFwdPackets        int64  \n",
            " 69  SubflowFwdBytes          int64  \n",
            " 70  SubflowBwdPackets        int64  \n",
            " 71  SubflowBwdBytes          int64  \n",
            " 72  Init_Win_bytes_forward   int64  \n",
            " 73  Init_Win_bytes_backward  int64  \n",
            " 74  act_data_pkt_fwd         int64  \n",
            " 75  min_seg_size_forward     int64  \n",
            " 76  ActiveMean               float64\n",
            " 77  ActiveStd                float64\n",
            " 78  ActiveMax                float64\n",
            " 79  ActiveMin                float64\n",
            " 80  IdleMean                 float64\n",
            " 81  IdleStd                  float64\n",
            " 82  IdleMax                  float64\n",
            " 83  IdleMin                  float64\n",
            " 84  Label                    object \n",
            "dtypes: float64(45), int64(35), object(5)\n",
            "memory usage: 1.6+ GB\n",
            "\n",
            "Testing dataset loaded successfully from ./test_data.csv\n",
            "Testing dataset shape: (639695, 85)\n",
            "\n",
            "First 5 rows of the testing dataset:\n",
            "                                    FlowID       SourceIP  SourcePort  \\\n",
            "0   192.168.10.5-211.233.74.132-58565-80-6   192.168.10.5       58565   \n",
            "1   192.168.10.3-192.168.10.14-53-52520-17  192.168.10.14       52520   \n",
            "2      172.16.0.1-192.168.10.50-39234-80-6     172.16.0.1       39234   \n",
            "3  192.168.10.25-104.97.133.94-55588-443-6  192.168.10.25       55588   \n",
            "4  157.240.18.35-192.168.10.15-443-54074-6  192.168.10.15       54074   \n",
            "\n",
            "    DestinationIP  DestinationPort  Protocol            Timestamp  \\\n",
            "0  211.233.74.132               80         6        6/7/2017 4:55   \n",
            "1    192.168.10.3               53        17        7/7/2017 1:57   \n",
            "2   192.168.10.50               80         6       5/7/2017 10:45   \n",
            "3   104.97.133.94              443         6  03/07/2017 10:55:48   \n",
            "4   157.240.18.35              443         6       4/7/2017 11:47   \n",
            "\n",
            "   FlowDuration  TotalFwdPackets  TotalBackwardPackets  ...  \\\n",
            "0       5658973                3                     1  ...   \n",
            "1           346                2                     2  ...   \n",
            "2             1                2                     0  ...   \n",
            "3         77778               11                     6  ...   \n",
            "4          5630                1                     1  ...   \n",
            "\n",
            "   min_seg_size_forward  ActiveMean  ActiveStd  ActiveMax  ActiveMin  \\\n",
            "0                    20         0.0        0.0        0.0        0.0   \n",
            "1                    20         0.0        0.0        0.0        0.0   \n",
            "2                    32         0.0        0.0        0.0        0.0   \n",
            "3                    32         0.0        0.0        0.0        0.0   \n",
            "4                    20         0.0        0.0        0.0        0.0   \n",
            "\n",
            "   IdleMean  IdleStd  IdleMax  IdleMin     Label  \n",
            "0       0.0      0.0      0.0      0.0    BENIGN  \n",
            "1       0.0      0.0      0.0      0.0    BENIGN  \n",
            "2       0.0      0.0      0.0      0.0  DoS Hulk  \n",
            "3       0.0      0.0      0.0      0.0    BENIGN  \n",
            "4       0.0      0.0      0.0      0.0    BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Testing Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 639695 entries, 0 to 639694\n",
            "Data columns (total 85 columns):\n",
            " #   Column                   Non-Null Count   Dtype  \n",
            "---  ------                   --------------   -----  \n",
            " 0   FlowID                   639695 non-null  object \n",
            " 1   SourceIP                 639695 non-null  object \n",
            " 2   SourcePort               639695 non-null  int64  \n",
            " 3   DestinationIP            639695 non-null  object \n",
            " 4   DestinationPort          639695 non-null  int64  \n",
            " 5   Protocol                 639695 non-null  int64  \n",
            " 6   Timestamp                639695 non-null  object \n",
            " 7   FlowDuration             639695 non-null  int64  \n",
            " 8   TotalFwdPackets          639695 non-null  int64  \n",
            " 9   TotalBackwardPackets     639695 non-null  int64  \n",
            " 10  TotalLengthofFwdPackets  639695 non-null  float64\n",
            " 11  TotalLengthofBwdPackets  639695 non-null  float64\n",
            " 12  FwdPacketLengthMax       639695 non-null  float64\n",
            " 13  FwdPacketLengthMin       639695 non-null  float64\n",
            " 14  FwdPacketLengthMean      639695 non-null  float64\n",
            " 15  FwdPacketLengthStd       639695 non-null  float64\n",
            " 16  BwdPacketLengthMax       639695 non-null  float64\n",
            " 17  BwdPacketLengthMin       639695 non-null  float64\n",
            " 18  BwdPacketLengthMean      639695 non-null  float64\n",
            " 19  BwdPacketLengthStd       639695 non-null  float64\n",
            " 20  FlowBytes/s              639695 non-null  float64\n",
            " 21  FlowPackets/s            639695 non-null  float64\n",
            " 22  FlowIATMean              639695 non-null  float64\n",
            " 23  FlowIATStd               639695 non-null  float64\n",
            " 24  FlowIATMax               639695 non-null  float64\n",
            " 25  FlowIATMin               639695 non-null  float64\n",
            " 26  FwdIATTotal              639695 non-null  float64\n",
            " 27  FwdIATMean               639695 non-null  float64\n",
            " 28  FwdIATStd                639695 non-null  float64\n",
            " 29  FwdIATMax                639695 non-null  float64\n",
            " 30  FwdIATMin                639695 non-null  float64\n",
            " 31  BwdIATTotal              639695 non-null  float64\n",
            " 32  BwdIATMean               639695 non-null  float64\n",
            " 33  BwdIATStd                639695 non-null  float64\n",
            " 34  BwdIATMax                639695 non-null  float64\n",
            " 35  BwdIATMin                639695 non-null  float64\n",
            " 36  FwdPSHFlags              639695 non-null  int64  \n",
            " 37  BwdPSHFlags              639695 non-null  int64  \n",
            " 38  FwdURGFlags              639695 non-null  int64  \n",
            " 39  BwdURGFlags              639695 non-null  int64  \n",
            " 40  FwdHeaderLength          639695 non-null  int64  \n",
            " 41  BwdHeaderLength          639695 non-null  int64  \n",
            " 42  FwdPackets/s             639695 non-null  float64\n",
            " 43  BwdPackets/s             639695 non-null  float64\n",
            " 44  MinPacketLength          639695 non-null  float64\n",
            " 45  MaxPacketLength          639695 non-null  float64\n",
            " 46  PacketLengthMean         639695 non-null  float64\n",
            " 47  PacketLengthStd          639695 non-null  float64\n",
            " 48  PacketLengthVariance     639695 non-null  float64\n",
            " 49  FINFlagCount             639695 non-null  int64  \n",
            " 50  SYNFlagCount             639695 non-null  int64  \n",
            " 51  RSTFlagCount             639695 non-null  int64  \n",
            " 52  PSHFlagCount             639695 non-null  int64  \n",
            " 53  ACKFlagCount             639695 non-null  int64  \n",
            " 54  URGFlagCount             639695 non-null  int64  \n",
            " 55  CWEFlagCount             639695 non-null  int64  \n",
            " 56  ECEFlagCount             639695 non-null  int64  \n",
            " 57  Down/UpRatio             639695 non-null  float64\n",
            " 58  AveragePacketSize        639695 non-null  float64\n",
            " 59  AvgFwdSegmentSize        639695 non-null  float64\n",
            " 60  AvgBwdSegmentSize        639695 non-null  float64\n",
            " 61  FwdHeaderLength.1        639695 non-null  int64  \n",
            " 62  FwdAvgBytes/Bulk         639695 non-null  int64  \n",
            " 63  FwdAvgPackets/Bulk       639695 non-null  int64  \n",
            " 64  FwdAvgBulkRate           639695 non-null  int64  \n",
            " 65  BwdAvgBytes/Bulk         639695 non-null  int64  \n",
            " 66  BwdAvgPackets/Bulk       639695 non-null  int64  \n",
            " 67  BwdAvgBulkRate           639695 non-null  int64  \n",
            " 68  SubflowFwdPackets        639695 non-null  int64  \n",
            " 69  SubflowFwdBytes          639695 non-null  int64  \n",
            " 70  SubflowBwdPackets        639695 non-null  int64  \n",
            " 71  SubflowBwdBytes          639695 non-null  int64  \n",
            " 72  Init_Win_bytes_forward   639695 non-null  int64  \n",
            " 73  Init_Win_bytes_backward  639695 non-null  int64  \n",
            " 74  act_data_pkt_fwd         639695 non-null  int64  \n",
            " 75  min_seg_size_forward     639695 non-null  int64  \n",
            " 76  ActiveMean               639695 non-null  float64\n",
            " 77  ActiveStd                639695 non-null  float64\n",
            " 78  ActiveMax                639695 non-null  float64\n",
            " 79  ActiveMin                639695 non-null  float64\n",
            " 80  IdleMean                 639695 non-null  float64\n",
            " 81  IdleStd                  639695 non-null  float64\n",
            " 82  IdleMax                  639695 non-null  float64\n",
            " 83  IdleMin                  639695 non-null  float64\n",
            " 84  Label                    639695 non-null  object \n",
            "dtypes: float64(45), int64(35), object(5)\n",
            "memory usage: 414.8+ MB\n",
            "\n",
            "Preprocessing applied to both training and testing datasets.\n",
            "Encoded value 0 -> Class label 'ATTACK'\n",
            "Encoded value 1 -> Class label 'BENIGN'\n",
            "\n",
            "Original Training Labels: ['BENIGN' 'ATTACK']\n",
            "Encoded Training Labels Classes: ['ATTACK' 'BENIGN']\n",
            "Encoded Training Labels: LabelEncoder()\n",
            "\n",
            "Original Testing Labels: ['BENIGN' 'ATTACK']\n",
            "Encoded Testing Labels (based on training labels): ['ATTACK' 'BENIGN']\n",
            "\n",
            "Label encoder saved to label_encoder.pkl\n",
            "\n",
            "Features scaled using StandardScaler (fitted on training data).\n",
            "Standard scaler saved to scaler.pkl\n",
            "\n",
            "Training and evaluating models...\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression training complete in 229.1468 seconds.\n",
            "Model 'Logistic Regression' saved to Logistic_Regression.pkl\n",
            "Logistic Regression prediction complete in 0.1681 seconds.\n",
            "Logistic Regression - Accuracy: 0.9393, Precision: 0.9389, Recall: 0.9393, F1-Score: 0.9391\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree training complete in 320.6224 seconds.\n",
            "Model 'Decision Tree' saved to Decision_Tree.pkl\n",
            "Decision Tree prediction complete in 0.2574 seconds.\n",
            "Decision Tree - Accuracy: 0.9998, Precision: 0.9998, Recall: 0.9998, F1-Score: 0.9998\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest training complete in 1267.1576 seconds.\n",
            "Model 'Random Forest' saved to Random_Forest.pkl\n",
            "Random Forest prediction complete in 9.5582 seconds.\n",
            "Random Forest - Accuracy: 0.9998, Precision: 0.9998, Recall: 0.9998, F1-Score: 0.9998\n",
            "\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:50:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost training complete in 91.0388 seconds.\n",
            "Model 'XGBoost' saved to XGBoost.pkl\n",
            "XGBoost prediction complete in 1.8965 seconds.\n",
            "XGBoost - Accuracy: 0.9999, Precision: 0.9999, Recall: 0.9999, F1-Score: 0.9999\n",
            "\n",
            "Training LightGBM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1979150, number of negative: 579630\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.246256 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 14858\n",
            "[LightGBM] [Info] Number of data points in the train set: 2558780, number of used features: 72\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.773474 -> initscore=1.228033\n",
            "[LightGBM] [Info] Start training from score 1.228033\n",
            "LightGBM training complete in 116.8101 seconds.\n",
            "Model 'LightGBM' saved to LightGBM.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM prediction complete in 4.5470 seconds.\n",
            "LightGBM - Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9997, F1-Score: 0.9997\n",
            "\n",
            "--- Model Comparison Results ---\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Accuracy: 0.9393\n",
            "Precision (Weighted): 0.9389\n",
            "Recall (Weighted): 0.9393\n",
            "F1-Score (Weighted): 0.9391\n",
            "Training Time (s): 229.1468\n",
            "Prediction Time (s): 0.1681\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       0.88      0.85      0.86    145426\n",
            "      BENIGN       0.96      0.96      0.96    494269\n",
            "\n",
            "    accuracy                           0.94    639695\n",
            "   macro avg       0.92      0.91      0.91    639695\n",
            "weighted avg       0.94      0.94      0.94    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[124146  21280]\n",
            " [ 17527 476742]]\n",
            "\n",
            "--- Decision Tree ---\n",
            "Accuracy: 0.9998\n",
            "Precision (Weighted): 0.9998\n",
            "Recall (Weighted): 0.9998\n",
            "F1-Score (Weighted): 0.9998\n",
            "Training Time (s): 320.6224\n",
            "Prediction Time (s): 0.2574\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       1.00      1.00      1.00    145426\n",
            "      BENIGN       1.00      1.00      1.00    494269\n",
            "\n",
            "    accuracy                           1.00    639695\n",
            "   macro avg       1.00      1.00      1.00    639695\n",
            "weighted avg       1.00      1.00      1.00    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145360     66]\n",
            " [    53 494216]]\n",
            "\n",
            "--- Random Forest ---\n",
            "Accuracy: 0.9998\n",
            "Precision (Weighted): 0.9998\n",
            "Recall (Weighted): 0.9998\n",
            "F1-Score (Weighted): 0.9998\n",
            "Training Time (s): 1267.1576\n",
            "Prediction Time (s): 9.5582\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       1.00      1.00      1.00    145426\n",
            "      BENIGN       1.00      1.00      1.00    494269\n",
            "\n",
            "    accuracy                           1.00    639695\n",
            "   macro avg       1.00      1.00      1.00    639695\n",
            "weighted avg       1.00      1.00      1.00    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145343     83]\n",
            " [    62 494207]]\n",
            "\n",
            "--- XGBoost ---\n",
            "Accuracy: 0.9999\n",
            "Precision (Weighted): 0.9999\n",
            "Recall (Weighted): 0.9999\n",
            "F1-Score (Weighted): 0.9999\n",
            "Training Time (s): 91.0388\n",
            "Prediction Time (s): 1.8965\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       1.00      1.00      1.00    145426\n",
            "      BENIGN       1.00      1.00      1.00    494269\n",
            "\n",
            "    accuracy                           1.00    639695\n",
            "   macro avg       1.00      1.00      1.00    639695\n",
            "weighted avg       1.00      1.00      1.00    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145401     25]\n",
            " [    49 494220]]\n",
            "\n",
            "--- LightGBM ---\n",
            "Accuracy: 0.9997\n",
            "Precision (Weighted): 0.9997\n",
            "Recall (Weighted): 0.9997\n",
            "F1-Score (Weighted): 0.9997\n",
            "Training Time (s): 116.8101\n",
            "Prediction Time (s): 4.5470\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       1.00      1.00      1.00    145426\n",
            "      BENIGN       1.00      1.00      1.00    494269\n",
            "\n",
            "    accuracy                           1.00    639695\n",
            "   macro avg       1.00      1.00      1.00    639695\n",
            "weighted avg       1.00      1.00      1.00    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145386     40]\n",
            " [   146 494123]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split # Still useful for potential validation split later, but not for initial train/test load\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier # Added SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier # Added BaggingClassifier (as an example of bagging)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB # Added Gaussian Naive Bayes\n",
        "from sklearn.neural_network import MLPClassifier # Added MLP Classifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import time\n",
        "import io # Keep io import in case needed for other things\n",
        "import pickle # Import the pickle library\n",
        "\n",
        "# Import Gradient Boosting Libraries (you might need to install these: pip install xgboost lightgbm)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBClassifier = xgb.XGBClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: XGBoost not installed. Skipping XGBoost model.\")\n",
        "    XGBClassifier = None # Set to None if not installed\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGBMClassifier = lgb.LGBMClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: LightGBM not installed. Skipping LightGBM model.\")\n",
        "    LGBMClassifier = None # Set to None if not installed\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace with the actual paths to your dataset files\n",
        "TRAIN_DATASET_PATH = './train_data.csv' # <--- Path to your training CSV file\n",
        "TEST_DATASET_PATH = './test_data.csv'   # <--- Path to your testing CSV file\n",
        "TARGET_COLUMN = 'Label' # The name of the target column\n",
        "\n",
        "# --- 1. Load Data from Separate CSV Files ---\n",
        "try:\n",
        "    df_train = pd.read_csv(TRAIN_DATASET_PATH)\n",
        "    print(f\"Training dataset loaded successfully from {TRAIN_DATASET_PATH}\")\n",
        "    print(f\"Training dataset shape: {df_train.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the training dataset:\")\n",
        "    print(df_train.head())\n",
        "    print(\"\\nTraining Dataset Info:\")\n",
        "    df_train.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Training dataset not found at {TRAIN_DATASET_PATH}\")\n",
        "    print(\"Please update TRAIN_DATASET_PATH with the correct path to your training CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "try:\n",
        "    df_test = pd.read_csv(TEST_DATASET_PATH)\n",
        "    print(f\"\\nTesting dataset loaded successfully from {TEST_DATASET_PATH}\")\n",
        "    print(f\"Testing dataset shape: {df_test.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the testing dataset:\")\n",
        "    print(df_test.head())\n",
        "    print(\"\\nTesting Dataset Info:\")\n",
        "    df_test.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Testing dataset not found at {TEST_DATASET_PATH}\")\n",
        "    print(\"Please update TEST_DATASET_PATH with the correct path to your testing CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "\n",
        "# --- 2. Preprocessing ---\n",
        "\n",
        "# Define columns to drop - apply to both train and test sets\n",
        "columns_to_drop = ['FlowID', 'SourceIP', 'DestinationIP', 'Timestamp']\n",
        "\n",
        "# Apply dropping columns and handling infinite/NaN values to both dataframes\n",
        "def preprocess_dataframe(df, columns_to_drop, target_column):\n",
        "    df_processed = df.drop(columns=columns_to_drop, errors='ignore').copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    # Handle potential infinite values\n",
        "    df_processed.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
        "    # Fill NaN values. Using 0 here, but consider other strategies.\n",
        "    df_processed.fillna(0, inplace=True)\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    X = df_processed.drop(columns=[target_column])\n",
        "    y = df_processed[target_column]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_dataframe(df_train, columns_to_drop, TARGET_COLUMN)\n",
        "X_test, y_test = preprocess_dataframe(df_test, columns_to_drop, TARGET_COLUMN)\n",
        "\n",
        "print(\"\\nPreprocessing applied to both training and testing datasets.\")\n",
        "\n",
        "# Encode the target variable if it's categorical (e.g., 'BENIGN', 'ATTACK_TYPE')\n",
        "# Fit on training labels, transform both training and testing labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test) # Use transform, not fit_transform on test set\n",
        "\n",
        "print(f\"\\nOriginal Training Labels: {y_train.unique()}\")\n",
        "print(f\"Encoded Training Labels: {label_encoder.classes_}\")\n",
        "print(f\"\\nOriginal Testing Labels: {y_test.unique()}\")\n",
        "print(f\"Encoded Testing Labels (based on training labels): {label_encoder.classes_}\")\n",
        "\n",
        "# --- Save the LabelEncoder ---\n",
        "try:\n",
        "    encoder_filename = 'label_encoder.pkl'\n",
        "    with open(encoder_filename, 'wb') as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "    print(f\"\\nLabel encoder saved to {encoder_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving label encoder: {e}\")\n",
        "\n",
        "\n",
        "# Scale numerical features\n",
        "# Fit scaler ONLY on training data, then transform both train and test data\n",
        "scaler = StandardScaler()\n",
        "# Check if training data is not empty before scaling\n",
        "if X_train.shape[0] > 0:\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    print(f\"\\nFeatures scaled using StandardScaler (fitted on training data).\")\n",
        "\n",
        "    # --- Save the StandardScaler ---\n",
        "    try:\n",
        "        scaler_filename = 'scaler.pkl'\n",
        "        with open(scaler_filename, 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        print(f\"Standard scaler saved to {scaler_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving standard scaler: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nWarning: Training data is empty after preprocessing. Cannot scale features or train models.\")\n",
        "    X_train_scaled = X_train # Keep as is if empty\n",
        "    X_test_scaled = X_test   # Keep as is if empty\n",
        "\n",
        "\n",
        "# --- 3. & 4. Model Selection and Training ---\n",
        "\n",
        "# Initialize different classifiers\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000), # Increased max_iter for convergence\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Support Vector Machine (Linear)\": SVC(kernel='linear', random_state=42),\n",
        "    \"Support Vector Machine (RBF)\": SVC(kernel='rbf', random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5), # Default n_neighbors is 5\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(), # Added Naive Bayes\n",
        "    \"SGD Classifier\": SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42), # Added SGD, using log_loss for logistic regression-like behavior\n",
        "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42), # Added MLP\n",
        "    \"Bagging Classifier (Decision Tree)\": BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42) # Fixed: Changed base_estimator to estimator\n",
        "}\n",
        "\n",
        "# Add Gradient Boosting models if installed\n",
        "if XGBClassifier is not None:\n",
        "    models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42) # Added XGBoost\n",
        "if LGBMClassifier is not None:\n",
        "    models[\"LightGBM\"] = LGBMClassifier(random_state=42) # Added LightGBM\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\nTraining and evaluating models...\")\n",
        "\n",
        "# Only attempt training and evaluation if there is sufficient training and testing data\n",
        "if X_train_scaled.shape[0] > 0 and X_test_scaled.shape[0] > 0 and len(label_encoder.classes_) > 0:\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        try:\n",
        "            model.fit(X_train_scaled, y_train_encoded)\n",
        "            end_time = time.time()\n",
        "            training_time = end_time - start_time\n",
        "            print(f\"{name} training complete in {training_time:.4f} seconds.\")\n",
        "\n",
        "            # --- Save the trained model using pickle ---\n",
        "            try:\n",
        "                # Create a safe filename from the model name\n",
        "                filename = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\") + \".pkl\"\n",
        "                with open(filename, 'wb') as f:\n",
        "                    pickle.dump(model, f)\n",
        "                print(f\"Model '{name}' saved to {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving model '{name}': {e}\")\n",
        "\n",
        "\n",
        "            # --- 5. Evaluation ---\n",
        "            start_time = time.time()\n",
        "            y_pred_encoded = model.predict(X_test_scaled)\n",
        "            end_time = time.time()\n",
        "            prediction_time = end_time - start_time\n",
        "            print(f\"{name} prediction complete in {prediction_time:.4f} seconds.\")\n",
        "\n",
        "            # Decode predictions back to original labels for clarity in report\n",
        "            # y_pred = label_encoder.inverse_transform(y_pred_encoded) # Decoding not strictly needed for metrics\n",
        "\n",
        "            accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "            try:\n",
        "                # Use classification_report for a comprehensive view\n",
        "                # zero_division=0 handles cases where a class has no true samples in the test set\n",
        "                report = classification_report(y_test_encoded, y_pred_encoded, target_names=label_encoder.classes_, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "                # Extract precision, recall, f1 from the report for easy printing\n",
        "                # This is a simplified way; parsing the report string might be needed for exact values\n",
        "                # For simplicity, we'll just print the full report.\n",
        "                # If you need specific average metrics (weighted, macro, micro), calculate them explicitly:\n",
        "                precision = precision_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                recall = recall_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                f1 = f1_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "\n",
        "\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Precision (Weighted)\": precision,\n",
        "                    \"Recall (Weighted)\": recall,\n",
        "                    \"F1-Score (Weighted)\": f1,\n",
        "                    \"Classification Report\": report,\n",
        "                    \"Confusion Matrix\": conf_matrix,\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "            except ValueError as e:\n",
        "                print(f\"Could not calculate all metrics for {name}. This might happen if test set has classes not in training set.\")\n",
        "                print(f\"Error details: {e}\")\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Classification Report\": \"Could not generate comprehensive report due to data limitations or missing classes in test set.\",\n",
        "                    \"Confusion Matrix\": \"Could not generate confusion matrix due to data limitations or missing classes in test set.\",\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training or evaluation of {name}: {e}\")\n",
        "            results[name] = {\n",
        "                \"Error\": str(e),\n",
        "                \"Training Time (s)\": training_time if 'training_time' in locals() else 'N/A',\n",
        "                \"Prediction Time (s)\": prediction_time if 'prediction_time' in locals() else 'N/A'\n",
        "            }\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping model training and evaluation due to insufficient data or classes.\")\n",
        "\n",
        "\n",
        "# --- 6. Comparison ---\n",
        "\n",
        "print(\"\\n--- Model Comparison Results ---\")\n",
        "\n",
        "# Print results in a readable format\n",
        "if results:\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        if \"Error\" in metrics:\n",
        "            print(f\"Error during processing: {metrics['Error']}\")\n",
        "            print(f\"Training Time (s): {metrics.get('Training Time (s)', 'N/A'):.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics.get('Prediction Time (s)', 'N/A'):.4f}\")\n",
        "        else:\n",
        "            print(f\"Accuracy: {metrics.get('Accuracy', 'N/A'):.4f}\")\n",
        "            # Check if other metrics were calculated before printing\n",
        "            if 'Precision (Weighted)' in metrics:\n",
        "                print(f\"Precision (Weighted): {metrics['Precision (Weighted)']:.4f}\")\n",
        "                print(f\"Recall (Weighted): {metrics['Recall (Weighted)']:.4f}\")\n",
        "                print(f\"F1-Score (Weighted): {metrics['F1-Score (Weighted)']:.4f}\")\n",
        "            print(f\"Training Time (s): {metrics['Training Time (s)']:.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics['Prediction Time (s)']:.4f}\")\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(metrics['Classification Report'])\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(metrics['Confusion Matrix'])\n",
        "else:\n",
        "    print(\"No results to display. Model training and evaluation were skipped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "8DCGdRcA70Zw",
        "outputId": "61083fd6-195b-41f2-96fd-dd0de3da5e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Training dataset not found at ./train_data.csv\n",
            "Please update TRAIN_DATASET_PATH with the correct path to your training CSV file.\n",
            "Error: Testing dataset not found at ./test_data.csv\n",
            "Please update TEST_DATASET_PATH with the correct path to your testing CSV file.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 84\u001b[0m\n\u001b[1;32m     80\u001b[0m     y \u001b[38;5;241m=\u001b[39m df_processed[target_column]\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n\u001b[0;32m---> 84\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m preprocess_dataframe(\u001b[43mdf_train\u001b[49m, columns_to_drop, TARGET_COLUMN)\n\u001b[1;32m     85\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m preprocess_dataframe(df_test, columns_to_drop, TARGET_COLUMN)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPreprocessing applied to both training and testing datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8holJvYw5ie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b0103cdf-d891-4520-dbd4-bfbd1fcc4315"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split \u001b[38;5;66;03m# Still useful for potential validation split later, but not for initial train/test load\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split # Still useful for potential validation split later, but not for initial train/test load\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier # Added SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier # Added BaggingClassifier (as an example of bagging)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB # Added Gaussian Naive Bayes\n",
        "from sklearn.neural_network import MLPClassifier # Added MLP Classifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import time\n",
        "import io # Keep io import in case needed for other things\n",
        "\n",
        "# Import Gradient Boosting Libraries (you might need to install these: pip install xgboost lightgbm)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBClassifier = xgb.XGBClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: XGBoost not installed. Skipping XGBoost model.\")\n",
        "    XGBClassifier = None # Set to None if not installed\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGBMClassifier = lgb.LGBMClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: LightGBM not installed. Skipping LightGBM model.\")\n",
        "    LGBMClassifier = None # Set to None if not installed\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace with the actual paths to your dataset files\n",
        "TRAIN_DATASET_PATH = './Dataset/train_data.csv' # <--- Path to your training CSV file\n",
        "TEST_DATASET_PATH = './Dataset/test_data.csv'   # <--- Path to your testing CSV file\n",
        "TARGET_COLUMN = 'Label' # The name of the target column\n",
        "\n",
        "# --- 1. Load Data from Separate CSV Files ---\n",
        "try:\n",
        "    df_train = pd.read_csv(TRAIN_DATASET_PATH)\n",
        "    print(f\"Training dataset loaded successfully from {TRAIN_DATASET_PATH}\")\n",
        "    print(f\"Training dataset shape: {df_train.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the training dataset:\")\n",
        "    print(df_train.head())\n",
        "    print(\"\\nTraining Dataset Info:\")\n",
        "    df_train.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Training dataset not found at {TRAIN_DATASET_PATH}\")\n",
        "    print(\"Please update TRAIN_DATASET_PATH with the correct path to your training CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "try:\n",
        "    df_test = pd.read_csv(TEST_DATASET_PATH)\n",
        "    print(f\"\\nTesting dataset loaded successfully from {TEST_DATASET_PATH}\")\n",
        "    print(f\"Testing dataset shape: {df_test.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the testing dataset:\")\n",
        "    print(df_test.head())\n",
        "    print(\"\\nTesting Dataset Info:\")\n",
        "    df_test.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Testing dataset not found at {TEST_DATASET_PATH}\")\n",
        "    print(\"Please update TEST_DATASET_PATH with the correct path to your testing CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "\n",
        "# --- 2. Preprocessing ---\n",
        "\n",
        "# Define columns to drop - apply to both train and test sets\n",
        "columns_to_drop = ['FlowID', 'SourceIP', 'DestinationIP', 'Timestamp']\n",
        "\n",
        "# Apply dropping columns and handling infinite/NaN values to both dataframes\n",
        "def preprocess_dataframe(df, columns_to_drop, target_column):\n",
        "    df_processed = df.drop(columns=columns_to_drop, errors='ignore').copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    # Handle potential infinite values\n",
        "    df_processed.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
        "    # Fill NaN values. Using 0 here, but consider other strategies.\n",
        "    df_processed.fillna(0, inplace=True)\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    X = df_processed.drop(columns=[target_column])\n",
        "    y = df_processed[target_column]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_dataframe(df_train, columns_to_drop, TARGET_COLUMN)\n",
        "X_test, y_test = preprocess_dataframe(df_test, columns_to_drop, TARGET_COLUMN)\n",
        "\n",
        "print(\"\\nPreprocessing applied to both training and testing datasets.\")\n",
        "\n",
        "# Encode the target variable if it's categorical (e.g., 'BENIGN', 'ATTACK_TYPE')\n",
        "# Fit on training labels, transform both training and testing labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test) # Use transform, not fit_transform on test set\n",
        "\n",
        "print(f\"\\nOriginal Training Labels: {y_train.unique()}\")\n",
        "print(f\"Encoded Training Labels: {label_encoder.classes_}\")\n",
        "print(f\"\\nOriginal Testing Labels: {y_test.unique()}\")\n",
        "print(f\"Encoded Testing Labels (based on training labels): {label_encoder.classes_}\")\n",
        "\n",
        "\n",
        "# Scale numerical features\n",
        "# Fit scaler ONLY on training data, then transform both train and test data\n",
        "scaler = StandardScaler()\n",
        "# Check if training data is not empty before scaling\n",
        "if X_train.shape[0] > 0:\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    print(f\"\\nFeatures scaled using StandardScaler (fitted on training data).\")\n",
        "else:\n",
        "    print(\"\\nWarning: Training data is empty after preprocessing. Cannot scale features or train models.\")\n",
        "    X_train_scaled = X_train # Keep as is if empty\n",
        "    X_test_scaled = X_test   # Keep as is if empty\n",
        "\n",
        "\n",
        "# --- 3. & 4. Model Selection and Training ---\n",
        "\n",
        "# Initialize different classifiers\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000), # Increased max_iter for convergence\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Support Vector Machine (Linear)\": SVC(kernel='linear', random_state=42),\n",
        "    \"Support Vector Machine (RBF)\": SVC(kernel='rbf', random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5), # Default n_neighbors is 5\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(), # Added Naive Bayes\n",
        "    \"SGD Classifier\": SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42), # Added SGD, using log_loss for logistic regression-like behavior\n",
        "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42), # Added MLP\n",
        "    \"Bagging Classifier (Decision Tree)\": BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42) # Fixed: Changed base_estimator to estimator\n",
        "}\n",
        "\n",
        "# Add Gradient Boosting models if installed\n",
        "if XGBClassifier is not None:\n",
        "    models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42) # Added XGBoost\n",
        "if LGBMClassifier is not None:\n",
        "    models[\"LightGBM\"] = LGBMClassifier(random_state=42) # Added LightGBM\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\nTraining and evaluating models...\")\n",
        "\n",
        "# Only attempt training and evaluation if there is sufficient training and testing data\n",
        "if X_train_scaled.shape[0] > 0 and X_test_scaled.shape[0] > 0 and len(label_encoder.classes_) > 0:\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        try:\n",
        "            model.fit(X_train_scaled, y_train_encoded)\n",
        "            end_time = time.time()\n",
        "            training_time = end_time - start_time\n",
        "            print(f\"{name} training complete in {training_time:.4f} seconds.\")\n",
        "\n",
        "            # --- 5. Evaluation ---\n",
        "            start_time = time.time()\n",
        "            y_pred_encoded = model.predict(X_test_scaled)\n",
        "            end_time = time.time()\n",
        "            prediction_time = end_time - start_time\n",
        "            print(f\"{name} prediction complete in {prediction_time:.4f} seconds.\")\n",
        "\n",
        "            # Decode predictions back to original labels for clarity in report\n",
        "            # y_pred = label_encoder.inverse_transform(y_pred_encoded) # Decoding not strictly needed for metrics\n",
        "\n",
        "            accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "            try:\n",
        "                # Use classification_report for a comprehensive view\n",
        "                # zero_division=0 handles cases where a class has no true samples in the test set\n",
        "                report = classification_report(y_test_encoded, y_pred_encoded, target_names=label_encoder.classes_, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "                # Extract precision, recall, f1 from the report for easy printing\n",
        "                # This is a simplified way; parsing the report string might be needed for exact values\n",
        "                # For simplicity, we'll just print the full report.\n",
        "                # If you need specific average metrics (weighted, macro, micro), calculate them explicitly:\n",
        "                precision = precision_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                recall = recall_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                f1 = f1_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "\n",
        "\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Precision (Weighted)\": precision,\n",
        "                    \"Recall (Weighted)\": recall,\n",
        "                    \"F1-Score (Weighted)\": f1,\n",
        "                    \"Classification Report\": report,\n",
        "                    \"Confusion Matrix\": conf_matrix,\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "            except ValueError as e:\n",
        "                print(f\"Could not calculate all metrics for {name}. This might happen if test set has classes not in training set.\")\n",
        "                print(f\"Error details: {e}\")\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Classification Report\": \"Could not generate comprehensive report due to data limitations or missing classes in test set.\",\n",
        "                    \"Confusion Matrix\": \"Could not generate confusion matrix due to data limitations or missing classes in test set.\",\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training or evaluation of {name}: {e}\")\n",
        "            results[name] = {\n",
        "                \"Error\": str(e),\n",
        "                \"Training Time (s)\": training_time if 'training_time' in locals() else 'N/A',\n",
        "                \"Prediction Time (s)\": prediction_time if 'prediction_time' in locals() else 'N/A'\n",
        "            }\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping model training and evaluation due to insufficient data or classes.\")\n",
        "\n",
        "\n",
        "# --- 6. Comparison ---\n",
        "\n",
        "print(\"\\n--- Model Comparison Results ---\")\n",
        "\n",
        "# Print results in a readable format\n",
        "if results:\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        if \"Error\" in metrics:\n",
        "            print(f\"Error during processing: {metrics['Error']}\")\n",
        "            print(f\"Training Time (s): {metrics.get('Training Time (s)', 'N/A'):.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics.get('Prediction Time (s)', 'N/A'):.4f}\")\n",
        "        else:\n",
        "            print(f\"Accuracy: {metrics.get('Accuracy', 'N/A'):.4f}\")\n",
        "            # Check if other metrics were calculated before printing\n",
        "            if 'Precision (Weighted)' in metrics:\n",
        "                print(f\"Precision (Weighted): {metrics['Precision (Weighted)']:.4f}\")\n",
        "                print(f\"Recall (Weighted): {metrics['Recall (Weighted)']:.4f}\")\n",
        "                print(f\"F1-Score (Weighted): {metrics['F1-Score (Weighted)']:.4f}\")\n",
        "            print(f\"Training Time (s): {metrics['Training Time (s)']:.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics['Prediction Time (s)']:.4f}\")\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(metrics['Classification Report'])\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(metrics['Confusion Matrix'])\n",
        "else:\n",
        "    print(\"No results to display. Model training and evaluation were skipped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgpZ-ibB-D3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}