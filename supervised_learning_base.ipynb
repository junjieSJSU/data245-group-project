{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier \n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import time\n",
        "import io \n",
        "import pickle \n",
        "import psutil\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Memory Usage Monitoring Helper ---\n",
        "process = psutil.Process(os.getpid())\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Returns current memory usage of the process in MB.\"\"\"\n",
        "    mem_info = process.memory_info()\n",
        "    return mem_info.rss / (1024 * 1024) # Convert bytes to MB\n",
        "\n",
        "print(f\"Initial memory usage: {get_memory_usage():.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Gradient Boosting Libraries (you might need to install these: pip install xgboost lightgbm)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBClassifier = xgb.XGBClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: XGBoost not installed. Skipping XGBoost model.\")\n",
        "    XGBClassifier = None # Set to None if not installed\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGBMClassifier = lgb.LGBMClassifier\n",
        "except ImportError:\n",
        "    print(\"Warning: LightGBM not installed. Skipping LightGBM model.\")\n",
        "    LGBMClassifier = None "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "TRAIN_DATASET_PATH = 'Dataset/train_data.csv'\n",
        "TEST_DATASET_PATH = 'Dataset/test_data.csv'\n",
        "TARGET_COLUMN = 'Label' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Load Data from Separate CSV Files ---\n",
        "print(f\"\\nMemory usage before loading data: {get_memory_usage():.2f} MB\")\n",
        "try:\n",
        "    df_train = pd.read_csv(TRAIN_DATASET_PATH)\n",
        "    print(f\"Training dataset loaded successfully from {TRAIN_DATASET_PATH}\")\n",
        "    print(f\"Training dataset shape: {df_train.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the training dataset:\")\n",
        "    print(df_train.head())\n",
        "    print(\"\\nTraining Dataset Info:\")\n",
        "    df_train.info()\n",
        "    print(f\"Memory usage after loading training data: {get_memory_usage():.2f} MB\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Training dataset not found at {TRAIN_DATASET_PATH}\")\n",
        "    print(\"Please update TRAIN_DATASET_PATH with the correct path to your training CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found\n",
        "\n",
        "try:\n",
        "    df_test = pd.read_csv(TEST_DATASET_PATH)\n",
        "    print(f\"\\nTesting dataset loaded successfully from {TEST_DATASET_PATH}\")\n",
        "    print(f\"Testing dataset shape: {df_test.shape}\")\n",
        "    print(\"\\nFirst 5 rows of the testing dataset:\")\n",
        "    print(df_test.head())\n",
        "    print(\"\\nTesting Dataset Info:\")\n",
        "    df_test.info()\n",
        "    print(f\"Memory usage after loading testing data: {get_memory_usage():.2f} MB\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Testing dataset not found at {TEST_DATASET_PATH}\")\n",
        "    print(\"Please update TEST_DATASET_PATH with the correct path to your testing CSV file.\")\n",
        "    exit() # Exiting for demonstration purposes if file not found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Preprocessing ---\n",
        "print(f\"\\nMemory usage before preprocessing: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "# Define columns to drop - apply to both train and test sets\n",
        "columns_to_drop = ['FlowID', 'SourceIP', 'DestinationIP', 'Timestamp']\n",
        "\n",
        "# Apply dropping columns and handling infinite/NaN values to both dataframes\n",
        "def preprocess_dataframe(df, columns_to_drop, target_column):\n",
        "    df_processed = df.drop(columns=columns_to_drop, errors='ignore').copy()\n",
        "    # Handle potential infinite values\n",
        "    df_processed.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
        "    df_processed.fillna(0, inplace=True)\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    X = df_processed.drop(columns=[target_column])\n",
        "    y = df_processed[target_column]\n",
        "\n",
        "    y = y.astype(str)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_dataframe(df_train, columns_to_drop, TARGET_COLUMN)\n",
        "X_test, y_test = preprocess_dataframe(df_test, columns_to_drop, TARGET_COLUMN)\n",
        "\n",
        "print(\"\\nPreprocessing applied to both training and testing datasets.\")\n",
        "print(f\"Memory usage after initial preprocessing: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "def simplify_labels(y):\n",
        "    return y.apply(lambda x: 'BENIGN' if x.upper() == 'BENIGN' else 'ATTACK')\n",
        "\n",
        "y_train = simplify_labels(y_train)\n",
        "y_test = simplify_labels(y_test)\n",
        "\n",
        "print(\"\\nLabels simplified to 'BENIGN' vs 'ATTACK'.\")\n",
        "print(f\"Memory usage after simplifying labels: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "# Encode the target variable if it's categorical (e.g., 'BENIGN', 'ATTACK_TYPE')\n",
        "# Fit on training labels, transform both training and testing labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"Encoded value {i} -> Class label '{class_name}'\")\n",
        "\n",
        "print(f\"\\nOriginal Training Labels: {y_train.unique()}\")\n",
        "print(f\"Encoded Training Labels Classes: {label_encoder.classes_}\")\n",
        "print(f\"Encoded Training Labels: {label_encoder}\")\n",
        "print(f\"\\nOriginal Testing Labels: {y_test.unique()}\")\n",
        "print(f\"Encoded Testing Labels (based on training labels): {label_encoder.classes_}\")\n",
        "\n",
        "# --- Save the LabelEncoder ---\n",
        "try:\n",
        "    encoder_filename = 'label_encoder.pkl'\n",
        "    with open(encoder_filename, 'wb') as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "    print(f\"\\nLabel encoder saved to {encoder_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving label encoder: {e}\")\n",
        "print(f\"Memory usage after encoding labels and saving encoder: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "# Scale numerical features\n",
        "# Fit scaler ONLY on training data, then transform both train and test data\n",
        "scaler = StandardScaler()\n",
        "# Check if training data is not empty before scaling\n",
        "if X_train.shape[0] > 0:\n",
        "    print(f\"\\nMemory usage before scaling features: {get_memory_usage():.2f} MB\")\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    print(f\"Features scaled using StandardScaler (fitted on training data).\")\n",
        "    print(f\"Memory usage after scaling features: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "    # --- Save the StandardScaler ---\n",
        "    try:\n",
        "        scaler_filename = 'scaler.pkl'\n",
        "        with open(scaler_filename, 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        print(f\"Standard scaler saved to {scaler_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving standard scaler: {e}\")\n",
        "    print(f\"Memory usage after saving scaler: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nWarning: Training data is empty after preprocessing. Cannot scale features or train models.\")\n",
        "    X_train_scaled = X_train # Keep as is if empty\n",
        "    X_test_scaled = X_test   # Keep as is if empty\n",
        "    print(f\"Memory usage with empty data (scaling skipped): {get_memory_usage():.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TkwQoLHtGD0",
        "outputId": "58418c90-964a-4a14-acfb-4fc318026c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial memory usage: 239.12 MB\n",
            "\n",
            "Memory usage before loading data: 323.76 MB\n",
            "Training dataset loaded successfully from ./train_data.csv\n",
            "Training dataset shape: (2558780, 85)\n",
            "\n",
            "First 5 rows of the training dataset:\n",
            "                                   FlowID       SourceIP  SourcePort  \\\n",
            "0  192.168.10.3-192.168.10.12-53-26526-17  192.168.10.12       26526   \n",
            "1   172.16.0.1-192.168.10.50-37255-3737-6     172.16.0.1       37255   \n",
            "2    192.168.10.16-72.21.91.29-53482-80-6  192.168.10.16       53482   \n",
            "3    192.168.10.15-31.13.71.7-50902-443-6     31.13.71.7         443   \n",
            "4   192.168.10.3-192.168.10.9-53-51576-17   192.168.10.9       51576   \n",
            "\n",
            "   DestinationIP  DestinationPort  Protocol            Timestamp  \\\n",
            "0   192.168.10.3               53        17        6/7/2017 3:12   \n",
            "1  192.168.10.50             3737         6        7/7/2017 2:52   \n",
            "2    72.21.91.29               80         6       5/7/2017 10:02   \n",
            "3  192.168.10.15            50902         6  03/07/2017 10:16:29   \n",
            "4   192.168.10.3               53        17        4/7/2017 4:59   \n",
            "\n",
            "   FlowDuration  TotalFwdPackets  TotalBackwardPackets  ...  \\\n",
            "0         31419                2                     2  ...   \n",
            "1            33                1                     1  ...   \n",
            "2       5713462                3                     1  ...   \n",
            "3           240                3                     1  ...   \n",
            "4           187                2                     2  ...   \n",
            "\n",
            "   min_seg_size_forward  ActiveMean  ActiveStd  ActiveMax  ActiveMin  \\\n",
            "0                    20         0.0        0.0        0.0        0.0   \n",
            "1                    24         0.0        0.0        0.0        0.0   \n",
            "2                    32         0.0        0.0        0.0        0.0   \n",
            "3                    20         0.0        0.0        0.0        0.0   \n",
            "4                    20         0.0        0.0        0.0        0.0   \n",
            "\n",
            "   IdleMean  IdleStd  IdleMax  IdleMin     Label  \n",
            "0       0.0      0.0      0.0      0.0    BENIGN  \n",
            "1       0.0      0.0      0.0      0.0  PortScan  \n",
            "2       0.0      0.0      0.0      0.0    BENIGN  \n",
            "3       0.0      0.0      0.0      0.0    BENIGN  \n",
            "4       0.0      0.0      0.0      0.0    BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Training Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2558780 entries, 0 to 2558779\n",
            "Data columns (total 85 columns):\n",
            " #   Column                   Dtype  \n",
            "---  ------                   -----  \n",
            " 0   FlowID                   object \n",
            " 1   SourceIP                 object \n",
            " 2   SourcePort               int64  \n",
            " 3   DestinationIP            object \n",
            " 4   DestinationPort          int64  \n",
            " 5   Protocol                 int64  \n",
            " 6   Timestamp                object \n",
            " 7   FlowDuration             int64  \n",
            " 8   TotalFwdPackets          int64  \n",
            " 9   TotalBackwardPackets     int64  \n",
            " 10  TotalLengthofFwdPackets  float64\n",
            " 11  TotalLengthofBwdPackets  float64\n",
            " 12  FwdPacketLengthMax       float64\n",
            " 13  FwdPacketLengthMin       float64\n",
            " 14  FwdPacketLengthMean      float64\n",
            " 15  FwdPacketLengthStd       float64\n",
            " 16  BwdPacketLengthMax       float64\n",
            " 17  BwdPacketLengthMin       float64\n",
            " 18  BwdPacketLengthMean      float64\n",
            " 19  BwdPacketLengthStd       float64\n",
            " 20  FlowBytes/s              float64\n",
            " 21  FlowPackets/s            float64\n",
            " 22  FlowIATMean              float64\n",
            " 23  FlowIATStd               float64\n",
            " 24  FlowIATMax               float64\n",
            " 25  FlowIATMin               float64\n",
            " 26  FwdIATTotal              float64\n",
            " 27  FwdIATMean               float64\n",
            " 28  FwdIATStd                float64\n",
            " 29  FwdIATMax                float64\n",
            " 30  FwdIATMin                float64\n",
            " 31  BwdIATTotal              float64\n",
            " 32  BwdIATMean               float64\n",
            " 33  BwdIATStd                float64\n",
            " 34  BwdIATMax                float64\n",
            " 35  BwdIATMin                float64\n",
            " 36  FwdPSHFlags              int64  \n",
            " 37  BwdPSHFlags              int64  \n",
            " 38  FwdURGFlags              int64  \n",
            " 39  BwdURGFlags              int64  \n",
            " 40  FwdHeaderLength          int64  \n",
            " 41  BwdHeaderLength          int64  \n",
            " 42  FwdPackets/s             float64\n",
            " 43  BwdPackets/s             float64\n",
            " 44  MinPacketLength          float64\n",
            " 45  MaxPacketLength          float64\n",
            " 46  PacketLengthMean         float64\n",
            " 47  PacketLengthStd          float64\n",
            " 48  PacketLengthVariance     float64\n",
            " 49  FINFlagCount             int64  \n",
            " 50  SYNFlagCount             int64  \n",
            " 51  RSTFlagCount             int64  \n",
            " 52  PSHFlagCount             int64  \n",
            " 53  ACKFlagCount             int64  \n",
            " 54  URGFlagCount             int64  \n",
            " 55  CWEFlagCount             int64  \n",
            " 56  ECEFlagCount             int64  \n",
            " 57  Down/UpRatio             float64\n",
            " 58  AveragePacketSize        float64\n",
            " 59  AvgFwdSegmentSize        float64\n",
            " 60  AvgBwdSegmentSize        float64\n",
            " 61  FwdHeaderLength.1        int64  \n",
            " 62  FwdAvgBytes/Bulk         int64  \n",
            " 63  FwdAvgPackets/Bulk       int64  \n",
            " 64  FwdAvgBulkRate           int64  \n",
            " 65  BwdAvgBytes/Bulk         int64  \n",
            " 66  BwdAvgPackets/Bulk       int64  \n",
            " 67  BwdAvgBulkRate           int64  \n",
            " 68  SubflowFwdPackets        int64  \n",
            " 69  SubflowFwdBytes          int64  \n",
            " 70  SubflowBwdPackets        int64  \n",
            " 71  SubflowBwdBytes          int64  \n",
            " 72  Init_Win_bytes_forward   int64  \n",
            " 73  Init_Win_bytes_backward  int64  \n",
            " 74  act_data_pkt_fwd         int64  \n",
            " 75  min_seg_size_forward     int64  \n",
            " 76  ActiveMean               float64\n",
            " 77  ActiveStd                float64\n",
            " 78  ActiveMax                float64\n",
            " 79  ActiveMin                float64\n",
            " 80  IdleMean                 float64\n",
            " 81  IdleStd                  float64\n",
            " 82  IdleMax                  float64\n",
            " 83  IdleMin                  float64\n",
            " 84  Label                    object \n",
            "dtypes: float64(45), int64(35), object(5)\n",
            "memory usage: 1.6+ GB\n",
            "Memory usage after loading training data: 3875.80 MB\n",
            "\n",
            "Testing dataset loaded successfully from ./test_data.csv\n",
            "Testing dataset shape: (639695, 85)\n",
            "\n",
            "First 5 rows of the testing dataset:\n",
            "                                    FlowID       SourceIP  SourcePort  \\\n",
            "0   192.168.10.5-211.233.74.132-58565-80-6   192.168.10.5       58565   \n",
            "1   192.168.10.3-192.168.10.14-53-52520-17  192.168.10.14       52520   \n",
            "2      172.16.0.1-192.168.10.50-39234-80-6     172.16.0.1       39234   \n",
            "3  192.168.10.25-104.97.133.94-55588-443-6  192.168.10.25       55588   \n",
            "4  157.240.18.35-192.168.10.15-443-54074-6  192.168.10.15       54074   \n",
            "\n",
            "    DestinationIP  DestinationPort  Protocol            Timestamp  \\\n",
            "0  211.233.74.132               80         6        6/7/2017 4:55   \n",
            "1    192.168.10.3               53        17        7/7/2017 1:57   \n",
            "2   192.168.10.50               80         6       5/7/2017 10:45   \n",
            "3   104.97.133.94              443         6  03/07/2017 10:55:48   \n",
            "4   157.240.18.35              443         6       4/7/2017 11:47   \n",
            "\n",
            "   FlowDuration  TotalFwdPackets  TotalBackwardPackets  ...  \\\n",
            "0       5658973                3                     1  ...   \n",
            "1           346                2                     2  ...   \n",
            "2             1                2                     0  ...   \n",
            "3         77778               11                     6  ...   \n",
            "4          5630                1                     1  ...   \n",
            "\n",
            "   min_seg_size_forward  ActiveMean  ActiveStd  ActiveMax  ActiveMin  \\\n",
            "0                    20         0.0        0.0        0.0        0.0   \n",
            "1                    20         0.0        0.0        0.0        0.0   \n",
            "2                    32         0.0        0.0        0.0        0.0   \n",
            "3                    32         0.0        0.0        0.0        0.0   \n",
            "4                    20         0.0        0.0        0.0        0.0   \n",
            "\n",
            "   IdleMean  IdleStd  IdleMax  IdleMin     Label  \n",
            "0       0.0      0.0      0.0      0.0    BENIGN  \n",
            "1       0.0      0.0      0.0      0.0    BENIGN  \n",
            "2       0.0      0.0      0.0      0.0  DoS Hulk  \n",
            "3       0.0      0.0      0.0      0.0    BENIGN  \n",
            "4       0.0      0.0      0.0      0.0    BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n",
            "\n",
            "Testing Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 639695 entries, 0 to 639694\n",
            "Data columns (total 85 columns):\n",
            " #   Column                   Non-Null Count   Dtype  \n",
            "---  ------                   --------------   -----  \n",
            " 0   FlowID                   639695 non-null  object \n",
            " 1   SourceIP                 639695 non-null  object \n",
            " 2   SourcePort               639695 non-null  int64  \n",
            " 3   DestinationIP            639695 non-null  object \n",
            " 4   DestinationPort          639695 non-null  int64  \n",
            " 5   Protocol                 639695 non-null  int64  \n",
            " 6   Timestamp                639695 non-null  object \n",
            " 7   FlowDuration             639695 non-null  int64  \n",
            " 8   TotalFwdPackets          639695 non-null  int64  \n",
            " 9   TotalBackwardPackets     639695 non-null  int64  \n",
            " 10  TotalLengthofFwdPackets  639695 non-null  float64\n",
            " 11  TotalLengthofBwdPackets  639695 non-null  float64\n",
            " 12  FwdPacketLengthMax       639695 non-null  float64\n",
            " 13  FwdPacketLengthMin       639695 non-null  float64\n",
            " 14  FwdPacketLengthMean      639695 non-null  float64\n",
            " 15  FwdPacketLengthStd       639695 non-null  float64\n",
            " 16  BwdPacketLengthMax       639695 non-null  float64\n",
            " 17  BwdPacketLengthMin       639695 non-null  float64\n",
            " 18  BwdPacketLengthMean      639695 non-null  float64\n",
            " 19  BwdPacketLengthStd       639695 non-null  float64\n",
            " 20  FlowBytes/s              639695 non-null  float64\n",
            " 21  FlowPackets/s            639695 non-null  float64\n",
            " 22  FlowIATMean              639695 non-null  float64\n",
            " 23  FlowIATStd               639695 non-null  float64\n",
            " 24  FlowIATMax               639695 non-null  float64\n",
            " 25  FlowIATMin               639695 non-null  float64\n",
            " 26  FwdIATTotal              639695 non-null  float64\n",
            " 27  FwdIATMean               639695 non-null  float64\n",
            " 28  FwdIATStd                639695 non-null  float64\n",
            " 29  FwdIATMax                639695 non-null  float64\n",
            " 30  FwdIATMin                639695 non-null  float64\n",
            " 31  BwdIATTotal              639695 non-null  float64\n",
            " 32  BwdIATMean               639695 non-null  float64\n",
            " 33  BwdIATStd                639695 non-null  float64\n",
            " 34  BwdIATMax                639695 non-null  float64\n",
            " 35  BwdIATMin                639695 non-null  float64\n",
            " 36  FwdPSHFlags              639695 non-null  int64  \n",
            " 37  BwdPSHFlags              639695 non-null  int64  \n",
            " 38  FwdURGFlags              639695 non-null  int64  \n",
            " 39  BwdURGFlags              639695 non-null  int64  \n",
            " 40  FwdHeaderLength          639695 non-null  int64  \n",
            " 41  BwdHeaderLength          639695 non-null  int64  \n",
            " 42  FwdPackets/s             639695 non-null  float64\n",
            " 43  BwdPackets/s             639695 non-null  float64\n",
            " 44  MinPacketLength          639695 non-null  float64\n",
            " 45  MaxPacketLength          639695 non-null  float64\n",
            " 46  PacketLengthMean         639695 non-null  float64\n",
            " 47  PacketLengthStd          639695 non-null  float64\n",
            " 48  PacketLengthVariance     639695 non-null  float64\n",
            " 49  FINFlagCount             639695 non-null  int64  \n",
            " 50  SYNFlagCount             639695 non-null  int64  \n",
            " 51  RSTFlagCount             639695 non-null  int64  \n",
            " 52  PSHFlagCount             639695 non-null  int64  \n",
            " 53  ACKFlagCount             639695 non-null  int64  \n",
            " 54  URGFlagCount             639695 non-null  int64  \n",
            " 55  CWEFlagCount             639695 non-null  int64  \n",
            " 56  ECEFlagCount             639695 non-null  int64  \n",
            " 57  Down/UpRatio             639695 non-null  float64\n",
            " 58  AveragePacketSize        639695 non-null  float64\n",
            " 59  AvgFwdSegmentSize        639695 non-null  float64\n",
            " 60  AvgBwdSegmentSize        639695 non-null  float64\n",
            " 61  FwdHeaderLength.1        639695 non-null  int64  \n",
            " 62  FwdAvgBytes/Bulk         639695 non-null  int64  \n",
            " 63  FwdAvgPackets/Bulk       639695 non-null  int64  \n",
            " 64  FwdAvgBulkRate           639695 non-null  int64  \n",
            " 65  BwdAvgBytes/Bulk         639695 non-null  int64  \n",
            " 66  BwdAvgPackets/Bulk       639695 non-null  int64  \n",
            " 67  BwdAvgBulkRate           639695 non-null  int64  \n",
            " 68  SubflowFwdPackets        639695 non-null  int64  \n",
            " 69  SubflowFwdBytes          639695 non-null  int64  \n",
            " 70  SubflowBwdPackets        639695 non-null  int64  \n",
            " 71  SubflowBwdBytes          639695 non-null  int64  \n",
            " 72  Init_Win_bytes_forward   639695 non-null  int64  \n",
            " 73  Init_Win_bytes_backward  639695 non-null  int64  \n",
            " 74  act_data_pkt_fwd         639695 non-null  int64  \n",
            " 75  min_seg_size_forward     639695 non-null  int64  \n",
            " 76  ActiveMean               639695 non-null  float64\n",
            " 77  ActiveStd                639695 non-null  float64\n",
            " 78  ActiveMax                639695 non-null  float64\n",
            " 79  ActiveMin                639695 non-null  float64\n",
            " 80  IdleMean                 639695 non-null  float64\n",
            " 81  IdleStd                  639695 non-null  float64\n",
            " 82  IdleMax                  639695 non-null  float64\n",
            " 83  IdleMin                  639695 non-null  float64\n",
            " 84  Label                    639695 non-null  object \n",
            "dtypes: float64(45), int64(35), object(5)\n",
            "memory usage: 414.8+ MB\n",
            "Memory usage after loading testing data: 4175.24 MB\n",
            "\n",
            "Memory usage before preprocessing: 4175.24 MB\n",
            "\n",
            "Preprocessing applied to both training and testing datasets.\n",
            "Memory usage after initial preprocessing: 5956.69 MB\n",
            "\n",
            "Labels simplified to 'BENIGN' vs 'ATTACK'.\n",
            "Memory usage after simplifying labels: 5956.69 MB\n",
            "Encoded value 0 -> Class label 'ATTACK'\n",
            "Encoded value 1 -> Class label 'BENIGN'\n",
            "\n",
            "Original Training Labels: ['BENIGN' 'ATTACK']\n",
            "Encoded Training Labels Classes: ['ATTACK' 'BENIGN']\n",
            "Encoded Training Labels: LabelEncoder()\n",
            "\n",
            "Original Testing Labels: ['BENIGN' 'ATTACK']\n",
            "Encoded Testing Labels (based on training labels): ['ATTACK' 'BENIGN']\n",
            "\n",
            "Label encoder saved to label_encoder.pkl\n",
            "Memory usage after encoding labels and saving encoder: 5956.69 MB\n",
            "\n",
            "Memory usage before scaling features: 5956.69 MB\n",
            "Features scaled using StandardScaler (fitted on training data).\n",
            "Memory usage after scaling features: 7902.44 MB\n",
            "Standard scaler saved to scaler.pkl\n",
            "Memory usage after saving scaler: 7902.44 MB\n",
            "\n",
            "Training and evaluating models...\n",
            "\n",
            "Memory usage before training XGBoost: 7902.44 MB\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:28:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost training complete in 95.1934 seconds.\n",
            "Memory usage after training XGBoost: 7927.92 MB\n",
            "Model 'XGBoost' saved to XGBoost.pkl\n",
            "Memory usage after saving model XGBoost: 7927.92 MB\n",
            "\n",
            "Memory usage before predicting with XGBoost: 7927.92 MB\n",
            "XGBoost prediction complete in 1.9096 seconds.\n",
            "Memory usage after predicting with XGBoost: 7927.92 MB\n",
            "XGBoost - Accuracy: 0.9999, Precision: 0.9999, Recall: 0.9999, F1-Score: 0.9999\n",
            "Memory usage after evaluating XGBoost: 7927.92 MB\n",
            "\n",
            "Memory usage before training LightGBM: 7927.92 MB\n",
            "Training LightGBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 1979150, number of negative: 579630\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.584952 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 14858\n",
            "[LightGBM] [Info] Number of data points in the train set: 2558780, number of used features: 72\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.773474 -> initscore=1.228033\n",
            "[LightGBM] [Info] Start training from score 1.228033\n",
            "LightGBM training complete in 120.7362 seconds.\n",
            "Memory usage after training LightGBM: 7888.56 MB\n",
            "Model 'LightGBM' saved to LightGBM.pkl\n",
            "Memory usage after saving model LightGBM: 7888.56 MB\n",
            "\n",
            "Memory usage before predicting with LightGBM: 7888.56 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightGBM prediction complete in 4.1356 seconds.\n",
            "Memory usage after predicting with LightGBM: 7889.56 MB\n",
            "LightGBM - Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9997, F1-Score: 0.9997\n",
            "Memory usage after evaluating LightGBM: 7889.56 MB\n",
            "\n",
            "--- Model Comparison Results ---\n",
            "\n",
            "--- XGBoost ---\n",
            "Accuracy: 0.9999\n",
            "Precision (Weighted): 0.9999\n",
            "Recall (Weighted): 0.9999\n",
            "F1-Score (Weighted): 0.9999\n",
            "Training Time (s): 95.1934\n",
            "Prediction Time (s): 1.9096\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       1.00      1.00      1.00    145426\n",
            "      BENIGN       1.00      1.00      1.00    494269\n",
            "\n",
            "    accuracy                           1.00    639695\n",
            "   macro avg       1.00      1.00      1.00    639695\n",
            "weighted avg       1.00      1.00      1.00    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145401     25]\n",
            " [    49 494220]]\n",
            "\n",
            "--- LightGBM ---\n",
            "Accuracy: 0.9997\n",
            "Precision (Weighted): 0.9997\n",
            "Recall (Weighted): 0.9997\n",
            "F1-Score (Weighted): 0.9997\n",
            "Training Time (s): 120.7362\n",
            "Prediction Time (s): 4.1356\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ATTACK       1.00      1.00      1.00    145426\n",
            "      BENIGN       1.00      1.00      1.00    494269\n",
            "\n",
            "    accuracy                           1.00    639695\n",
            "   macro avg       1.00      1.00      1.00    639695\n",
            "weighted avg       1.00      1.00      1.00    639695\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145386     40]\n",
            " [   146 494123]]\n",
            "\n",
            "Final memory usage before script finishes: 7889.56 MB\n"
          ]
        }
      ],
      "source": [
        "# --- 3. & 4. Model Selection and Training ---\n",
        "\n",
        "# Initialize different classifiers\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000), # Increased max_iter for convergence\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5), # Default n_neighbors is 5\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(), # Added Naive Bayes\n",
        "    # \"SGD Classifier\": SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42), # Added SGD, using log_loss for logistic regression-like behavior\n",
        "    # \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42), # Added MLP\n",
        "    \"Bagging Classifier (Decision Tree)\": BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42) # Fixed: Changed base_estimator to estimator\n",
        "    # \"Support Vector Machine (Linear)\": SVC(kernel='linear', random_state=42),\n",
        "    # \"Support Vector Machine (RBF)\": SVC(kernel='rbf', random_state=42),\n",
        "}\n",
        "\n",
        "# Add Gradient Boosting models if installed\n",
        "if XGBClassifier is not None:\n",
        "    models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42) # Added XGBoost\n",
        "if LGBMClassifier is not None:\n",
        "    models[\"LightGBM\"] = LGBMClassifier(random_state=42) # Added LightGBM\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\nTraining and evaluating models...\")\n",
        "\n",
        "# Only attempt training and evaluation if there is sufficient training and testing data\n",
        "if X_train_scaled.shape[0] > 0 and X_test_scaled.shape[0] > 0 and len(label_encoder.classes_) > 0:\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nMemory usage before training {name}: {get_memory_usage():.2f} MB\")\n",
        "        start_time = time.time()\n",
        "        print(f\"Training {name}...\")\n",
        "        try:\n",
        "            model.fit(X_train_scaled, y_train_encoded)\n",
        "            end_time = time.time()\n",
        "            training_time = end_time - start_time\n",
        "            print(f\"{name} training complete in {training_time:.4f} seconds.\")\n",
        "            print(f\"Memory usage after training {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "            # --- Save the trained model using pickle ---\n",
        "            try:\n",
        "                # Create a safe filename from the model name\n",
        "                filename = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\") + \".pkl\"\n",
        "                with open(filename, 'wb') as f:\n",
        "                    pickle.dump(model, f)\n",
        "                print(f\"Model '{name}' saved to {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving model '{name}': {e}\")\n",
        "            print(f\"Memory usage after saving model {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "            # --- 5. Evaluation ---\n",
        "            print(f\"\\nMemory usage before predicting with {name}: {get_memory_usage():.2f} MB\")\n",
        "            start_time = time.time()\n",
        "            y_pred_encoded = model.predict(X_test_scaled)\n",
        "            end_time = time.time()\n",
        "            prediction_time = end_time - start_time\n",
        "            print(f\"{name} prediction complete in {prediction_time:.4f} seconds.\")\n",
        "            print(f\"Memory usage after predicting with {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "            accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "            try:\n",
        "                # Use classification_report for a comprehensive view\n",
        "                # zero_division=0 handles cases where a class has no true samples in the test set\n",
        "                report = classification_report(y_test_encoded, y_pred_encoded, target_names=label_encoder.classes_, zero_division=0)\n",
        "                conf_matrix = confusion_matrix(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "                # Extract precision, recall, f1 from the report for easy printing\n",
        "                precision = precision_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                recall = recall_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "                f1 = f1_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "\n",
        "\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Precision (Weighted)\": precision,\n",
        "                    \"Recall (Weighted)\": recall,\n",
        "                    \"F1-Score (Weighted)\": f1,\n",
        "                    \"Classification Report\": report,\n",
        "                    \"Confusion Matrix\": conf_matrix,\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "                print(f\"Memory usage after evaluating {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"Could not calculate all metrics for {name}. This might happen if test set has classes not in training set.\")\n",
        "                print(f\"Error details: {e}\")\n",
        "                results[name] = {\n",
        "                    \"Accuracy\": accuracy,\n",
        "                    \"Classification Report\": \"Could not generate comprehensive report due to data limitations or missing classes in test set.\",\n",
        "                    \"Confusion Matrix\": \"Could not generate confusion matrix due to data limitations or missing classes in test set.\",\n",
        "                    \"Training Time (s)\": training_time,\n",
        "                    \"Prediction Time (s)\": prediction_time\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
        "                print(f\"Memory usage after partial evaluation of {name}: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training or evaluation of {name}: {e}\")\n",
        "            results[name] = {\n",
        "                \"Error\": str(e),\n",
        "                \"Training Time (s)\": training_time if 'training_time' in locals() else 'N/A',\n",
        "                \"Prediction Time (s)\": prediction_time if 'prediction_time' in locals() else 'N/A'\n",
        "            }\n",
        "            print(f\"Memory usage after error during {name} processing: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping model training and evaluation due to insufficient data or classes.\")\n",
        "    print(f\"Memory usage at the end of the script (training skipped): {get_memory_usage():.2f} MB\")\n",
        "\n",
        "\n",
        "# --- 6. Comparison ---\n",
        "\n",
        "print(\"\\n--- Model Comparison Results ---\")\n",
        "\n",
        "# Print results in a readable format\n",
        "if results:\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        if \"Error\" in metrics:\n",
        "            print(f\"Error during processing: {metrics['Error']}\")\n",
        "            print(f\"Training Time (s): {metrics.get('Training Time (s)', 'N/A'):.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics.get('Prediction Time (s)', 'N/A'):.4f}\")\n",
        "        else:\n",
        "            print(f\"Accuracy: {metrics.get('Accuracy', 'N/A'):.4f}\")\n",
        "            # Check if other metrics were calculated before printing\n",
        "            if 'Precision (Weighted)' in metrics:\n",
        "                print(f\"Precision (Weighted): {metrics['Precision (Weighted)']:.4f}\")\n",
        "                print(f\"Recall (Weighted): {metrics['Recall (Weighted)']:.4f}\")\n",
        "                print(f\"F1-Score (Weighted): {metrics['F1-Score (Weighted)']:.4f}\")\n",
        "            print(f\"Training Time (s): {metrics['Training Time (s)']:.4f}\")\n",
        "            print(f\"Prediction Time (s): {metrics['Prediction Time (s)']:.4f}\")\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(metrics['Classification Report'])\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(metrics['Confusion Matrix'])\n",
        "else:\n",
        "    print(\"No results to display. Model training and evaluation were skipped.\")\n",
        "\n",
        "print(f\"\\nFinal memory usage before script finishes: {get_memory_usage():.2f} MB\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
